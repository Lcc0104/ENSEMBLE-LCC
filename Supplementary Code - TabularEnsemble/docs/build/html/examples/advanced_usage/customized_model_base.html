
<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Customized model base &#8212; Tabular Ensemble 0.2 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx_paramlinks.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/nbsphinx-code-cells.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/nbsphinx_dataframe.css?v=60cbb005" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script src="../../_static/documentation_options.js?v=3b889da3"></script>
    <script src="../../_static/doctools.js?v=888ff710"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'examples/advanced_usage/customized_model_base';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Advanced customized model base" href="customized_model_base_advanced.html" />
    <link rel="prev" title="Advanced Usage" href="../advanced_usage.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  <div class="navbar-header-items__start">
    
      <div class="navbar-item">
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    <p class="title logo__title">Tabular Ensemble 0.2 documentation</p>
  
</a></div>
    
  </div>
  
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../get_started.html">
                        Get Started
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="../advanced_usage.html">
                        Advanced Usage
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../api/api.html">
                        API References
                      </a>
                    </li>
                
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
        </div>
      
      
        <div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/ANONYMOUS/tabular_ensemble" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-brands fa-github"></i></span>
            <label class="sr-only">GitHub</label></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
    </div>
  

  
    <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fa-solid fa-outdent"></span>
    </label>
  
</div>

    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../get_started.html">
                        Get Started
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="../advanced_usage.html">
                        Advanced Usage
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../api/api.html">
                        API References
                      </a>
                    </li>
                
  </ul>
</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/ANONYMOUS/tabular_ensemble" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-brands fa-github"></i></span>
            <label class="sr-only">GitHub</label></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item"><nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Customized model base</a></li>
<li class="toctree-l1"><a class="reference internal" href="customized_model_base_advanced.html">Advanced customized model base</a></li>
<li class="toctree-l1"><a class="reference internal" href="set_optimizers_lrschedulers.html">Change optimizers and lr shedulers</a></li>
<li class="toctree-l1"><a class="reference internal" href="build_model_upon_others.html">Build your own model upon others</a></li>
<li class="toctree-l1"><a class="reference internal" href="new_data_splitters.html">New data splitters</a></li>
<li class="toctree-l1"><a class="reference internal" href="new_data_derivers.html">New data derivers</a></li>
<li class="toctree-l1"><a class="reference internal" href="new_data_imputers.html">New data imputers</a></li>
<li class="toctree-l1"><a class="reference internal" href="new_data_processors.html">New data processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="image_classification.html">Multimodal data: Image classification as an example</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumbs">
  <ul class="bd-breadcrumbs" role="navigation" aria-label="Breadcrumb">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../advanced_usage.html" class="nav-link">Advanced Usage</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Customized model base</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="Customized-model-base">
<h1>Customized model base<a class="headerlink" href="#Customized-model-base" title="Link to this heading">#</a></h1>
<p>For researchers or model base developers, the basic need is comparing their own models with existing benchmarks in <code class="docutils literal notranslate"><span class="pre">tabensemb</span></code>. In this part, a model base is built within the framework assuming that we want to integrate <code class="docutils literal notranslate"><span class="pre">TabNet</span></code> (<a class="reference external" href="https://github.com/dreamquark-ai/tabnet">from dreamquark-ai team</a>) into <code class="docutils literal notranslate"><span class="pre">tabensemb</span></code> (indeed <code class="docutils literal notranslate"><span class="pre">pytorch_tabular</span></code> and <code class="docutils literal notranslate"><span class="pre">pytorch_widedeep</span></code> have done that) for regression and classification tasks.</p>
<p><strong>Remark</strong>: For <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code>-based models, we have implemented most requirements of the framework so that users can integrate <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>s more conveniently.</p>
<section id="Example:-Implement-TabNet-as-a-model-base-from-scratch">
<h2>Example: Implement TabNet as a model base from scratch<a class="headerlink" href="#Example:-Implement-TabNet-as-a-model-base-from-scratch" title="Link to this heading">#</a></h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tabensemb</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">tempfile</span> <span class="kn">import</span> <span class="n">TemporaryDirectory</span>

<span class="n">temp_path</span> <span class="o">=</span> <span class="n">TemporaryDirectory</span><span class="p">()</span>
<span class="n">tabensemb</span><span class="o">.</span><span class="n">setting</span><span class="p">[</span><span class="s2">&quot;default_output_path&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">temp_path</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">)</span>
<span class="n">tabensemb</span><span class="o">.</span><span class="n">setting</span><span class="p">[</span><span class="s2">&quot;default_config_path&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">temp_path</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;configs&quot;</span><span class="p">)</span>
<span class="n">tabensemb</span><span class="o">.</span><span class="n">setting</span><span class="p">[</span><span class="s2">&quot;default_data_path&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">temp_path</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;data&quot;</span><span class="p">)</span>

<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
</pre></div>
</div>
</div>
<p>All model bases inherit <code class="docutils literal notranslate"><span class="pre">AbstractModel</span></code> and implement methods within the class. If necessary methods are not implemented, <code class="docutils literal notranslate"><span class="pre">NotImplementedError</span></code> will be raised during usage.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tabensemb.model</span> <span class="kn">import</span> <span class="n">AbstractModel</span>
</pre></div>
</div>
</div>
<p>We use <code class="docutils literal notranslate"><span class="pre">scikit-optimize</span></code> (<a class="github reference external" href="https://github.com/scikit-optimize/scikit-optimize">scikit-optimize/scikit-optimize</a>) to do Bayesian hyperparameter optimization, so space classes are imported.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">skopt.space</span> <span class="kn">import</span> <span class="n">Integer</span><span class="p">,</span> <span class="n">Real</span><span class="p">,</span> <span class="n">Categorical</span>
</pre></div>
</div>
</div>
<p>First, we define the initialization of the model base. Always remember to pass all args and kwargs to <code class="docutils literal notranslate"><span class="pre">__init__</span></code> of <code class="docutils literal notranslate"><span class="pre">AbstractModel</span></code>. You can do other things in <code class="docutils literal notranslate"><span class="pre">__init__</span></code>. All <code class="docutils literal notranslate"><span class="pre">*args</span></code> and <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> (including arguments like the <code class="docutils literal notranslate"><span class="pre">some_param</span></code> shown below) are recorded in <code class="docutils literal notranslate"><span class="pre">self.init_params</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TabNetFromAbstract</span><span class="p">(</span><span class="n">AbstractModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">some_param</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TabNetFromAbstract</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="c1"># Do something else here</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">some_param</span> <span class="o">=</span> <span class="n">some_param</span>
        <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_params</span><span class="p">)</span>
</pre></div>
</div>
<p>We should define the name of the model base and all available models in the model base.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_get_program_name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="s2">&quot;TabNetFromAbstract&quot;</span>

<span class="k">def</span> <span class="nf">_get_model_names</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="s2">&quot;TabNet&quot;</span><span class="p">]</span>
</pre></div>
</div>
<p>For each model in the model base, the program will request initial hyperparameters of the model and their search spaces. They are defined as</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_space</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="n">Integer</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;n_d&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">),</span>
        <span class="n">Integer</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;n_a&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">),</span>
        <span class="n">Integer</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;n_steps&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">),</span>
        <span class="n">Real</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;gamma&quot;</span><span class="p">),</span>
        <span class="n">Integer</span><span class="p">(</span>
            <span class="n">low</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;n_independent&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span>
        <span class="p">),</span>
        <span class="n">Integer</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;n_shared&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">),</span>
    <span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">SPACE</span>

<span class="k">def</span> <span class="nf">_initial_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;n_d&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
        <span class="s2">&quot;n_a&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
        <span class="s2">&quot;n_steps&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="mf">1.3</span><span class="p">,</span>
        <span class="s2">&quot;n_independent&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="s2">&quot;n_shared&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">],</span>
        <span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;weight_decay&quot;</span><span class="p">],</span>
        <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">],</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>Before training, each model base has its own way of processing the dataset.</p>
<p><code class="docutils literal notranslate"><span class="pre">_train_data_preprocess</span></code> will return the processed dataset according to a given <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> which provides all training information and data required. In this example, <code class="docutils literal notranslate"><span class="pre">X_train/X_val/X_test</span></code> represent training/validation/testing sets, and <code class="docutils literal notranslate"><span class="pre">y_train/y_val/y_test</span></code> represent corresponding labels.</p>
<p><strong>Remark</strong>: The tabular dataset has gone through all processing stages defined in the <code class="docutils literal notranslate"><span class="pre">DataModule</span></code> inside the trainer <strong>except scaling</strong>. Call <code class="docutils literal notranslate"><span class="pre">self.trainer.datamodule.data_transform(df,</span> <span class="pre">scaler_only=True)</span></code> to scale it using the trained scaler if no scaling stage is defined internally in the model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_train_data_preprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">datamodule</span>
    <span class="n">all_feature_names</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">all_feature_names</span>

    <span class="n">X_train</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data_transform</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">X_train</span><span class="p">,</span> <span class="n">scaler_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span>
        <span class="n">all_feature_names</span>
    <span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
    <span class="n">X_val</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data_transform</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">X_val</span><span class="p">,</span> <span class="n">scaler_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span>
        <span class="n">all_feature_names</span>
    <span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data_transform</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">X_test</span><span class="p">,</span> <span class="n">scaler_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span>
        <span class="n">all_feature_names</span>
    <span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">y_train</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
    <span class="n">y_val</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">y_val</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
    <span class="n">y_test</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">y_test</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;X_train&quot;</span><span class="p">:</span> <span class="n">X_train</span><span class="p">,</span>
        <span class="s2">&quot;y_train&quot;</span><span class="p">:</span> <span class="n">y_train</span><span class="p">,</span>
        <span class="s2">&quot;X_val&quot;</span><span class="p">:</span> <span class="n">X_val</span><span class="p">,</span>
        <span class="s2">&quot;y_val&quot;</span><span class="p">:</span> <span class="n">y_val</span><span class="p">,</span>
        <span class="s2">&quot;X_test&quot;</span><span class="p">:</span> <span class="n">X_test</span><span class="p">,</span>
        <span class="s2">&quot;y_test&quot;</span><span class="p">:</span> <span class="n">y_test</span><span class="p">,</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>Correspondingly, <code class="docutils literal notranslate"><span class="pre">_data_preprocess</span></code> will process an upcoming new dataset, including the tabular data <code class="docutils literal notranslate"><span class="pre">df</span></code> containing continuous features and categorical features, and unstacked derived data <code class="docutils literal notranslate"><span class="pre">derived_data</span></code> (multi-modal data or something else depending on the configuration introduced in “Using data functionalities”). The returned value should have the same structure as the <code class="docutils literal notranslate"><span class="pre">X_test</span></code> returned in <code class="docutils literal notranslate"><span class="pre">_train_data_preprocess</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_data_preprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">derived_data</span><span class="p">,</span> <span class="n">model_name</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">datamodule</span><span class="o">.</span><span class="n">data_transform</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">scaler_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">all_feature_names</span>
    <span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
</pre></div>
</div>
<p>The program will pass a selected set of hyperparameters as <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> to initialize a model, train a model, and predict using the model. The returned <code class="docutils literal notranslate"><span class="pre">model</span></code> will be stored locally and reloaded for evaluation and inference, so make sure it contains all the information needed to make predictions.</p>
<p>Here we initialize the model using information contained in the <code class="docutils literal notranslate"><span class="pre">DataModule</span></code> instance, including the indices of categorical features <code class="docutils literal notranslate"><span class="pre">cat_idxs</span></code>, the number of categories of each categorical feature <code class="docutils literal notranslate"><span class="pre">cat_dims</span></code>, the current task <code class="docutils literal notranslate"><span class="pre">task</span></code> (possible values are “regression”, “binary”, and “multiclass”), the device to train the model <code class="docutils literal notranslate"><span class="pre">self.trainer.device</span></code>, and the hyperparameters <code class="docutils literal notranslate"><span class="pre">kwargs</span></code>. <code class="docutils literal notranslate"><span class="pre">model_name</span></code> is ignored because we only have one model in the model base. All model bases should at
least follow the guidance of <code class="docutils literal notranslate"><span class="pre">self.trainer.device</span></code>, <code class="docutils literal notranslate"><span class="pre">self.trainer.datamodule.task</span></code>, <code class="docutils literal notranslate"><span class="pre">model_name</span></code>, and <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> to make all models trained in a consistent way within the framework.</p>
<p><strong>Remark</strong>: In <code class="docutils literal notranslate"><span class="pre">DataModule.cat_num_unique</span></code> and <code class="docutils literal notranslate"><span class="pre">DataModule.cat_feature_mapping</span></code>, the category of unknown or missing values is already included as <code class="docutils literal notranslate"><span class="pre">-1</span></code> for integer-like categorical features and <code class="docutils literal notranslate"><span class="pre">UNK</span></code> for string-like categorical features.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_new_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">verbose</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">pytorch_tabnet.tab_model</span> <span class="kn">import</span> <span class="n">TabNetRegressor</span><span class="p">,</span> <span class="n">TabNetClassifier</span>

    <span class="n">datamodule</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">datamodule</span>
    <span class="n">cat_idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">datamodule</span><span class="o">.</span><span class="n">cont_feature_names</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">datamodule</span><span class="o">.</span><span class="n">all_feature_names</span><span class="p">)))</span>
    <span class="n">cat_dims</span> <span class="o">=</span> <span class="n">datamodule</span><span class="o">.</span><span class="n">cat_num_unique</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">task</span> <span class="o">=</span> <span class="n">datamodule</span><span class="o">.</span><span class="n">task</span>
    <span class="n">init_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">verbose</span><span class="o">=</span><span class="n">tabensemb</span><span class="o">.</span><span class="n">setting</span><span class="p">[</span><span class="s2">&quot;verbose_per_epoch&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="n">verbose</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">optimizer_params</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">],</span>
            <span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;weight_decay&quot;</span><span class="p">],</span>
        <span class="p">},</span>
        <span class="n">cat_idxs</span><span class="o">=</span><span class="n">cat_idxs</span><span class="p">,</span>
        <span class="n">cat_dims</span><span class="o">=</span><span class="n">cat_dims</span><span class="p">,</span>
        <span class="n">cat_emb_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">device_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">datamodule</span><span class="o">.</span><span class="n">task</span> <span class="o">==</span> <span class="s2">&quot;regression&quot;</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">TabNetRegressor</span><span class="p">(</span><span class="o">**</span><span class="n">init_kwargs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">TabNetClassifier</span><span class="p">(</span><span class="o">**</span><span class="n">init_kwargs</span><span class="p">)</span>

    <span class="n">model</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span>
        <span class="o">**</span><span class="p">{</span>
            <span class="s2">&quot;n_d&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;n_d&quot;</span><span class="p">],</span>
            <span class="s2">&quot;n_a&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;n_a&quot;</span><span class="p">],</span>
            <span class="s2">&quot;n_steps&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;n_steps&quot;</span><span class="p">],</span>
            <span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;gamma&quot;</span><span class="p">],</span>
            <span class="s2">&quot;n_independent&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;n_independent&quot;</span><span class="p">],</span>
            <span class="s2">&quot;n_shared&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;n_shared&quot;</span><span class="p">],</span>
        <span class="p">}</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
<p><strong>Remark</strong>: <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> has all keys defined in <code class="docutils literal notranslate"><span class="pre">_initial_values</span></code>. If a parameter named <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> is included, a new key named <code class="docutils literal notranslate"><span class="pre">original_batch_size</span></code> exists in <code class="docutils literal notranslate"><span class="pre">kwargs</span></code>. The values of <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> and <code class="docutils literal notranslate"><span class="pre">original_batch_size</span></code> may be different if the program finds that the batch size will make the mini-batches tiny. The threshold is defined by <code class="docutils literal notranslate"><span class="pre">self.limit_batch_size</span></code> (default to 6). A tiny batch might interrupt some models, so it is better to use the modified <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> value.</p>
<p>The framework will pass <code class="docutils literal notranslate"><span class="pre">X_train</span></code>, <code class="docutils literal notranslate"><span class="pre">y_train</span></code>, <code class="docutils literal notranslate"><span class="pre">X_val</span></code>, and <code class="docutils literal notranslate"><span class="pre">y_val</span></code> from <code class="docutils literal notranslate"><span class="pre">_train_data_preprocess</span></code> to the following <code class="docutils literal notranslate"><span class="pre">_train_single_model</span></code> method, along with some other arguments stating the current training stage. <code class="docutils literal notranslate"><span class="pre">epoch</span></code> is the number of epochs to train the model. <code class="docutils literal notranslate"><span class="pre">warm_start=True</span></code> means the passed model is already trained and should be fine-tuned based on a new dataset. <code class="docutils literal notranslate"><span class="pre">in_bayes_opt=True</span></code> means that the passed <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> is selected by a bayesian hyperparameter optimization
step, and a simplified training routine is needed to reduce optimization time, so we set the <code class="docutils literal notranslate"><span class="pre">max_epochs</span></code> to “bayes_epoch” in the configuration.</p>
<p><strong>Remark</strong>: <code class="docutils literal notranslate"><span class="pre">epoch</span></code> will be <code class="docutils literal notranslate"><span class="pre">self.trainer.args[&quot;bayes_epoch&quot;]</span></code> if <code class="docutils literal notranslate"><span class="pre">in_bayes_opt=True</span></code>, and <code class="docutils literal notranslate"><span class="pre">self.trainer.args[&quot;epoch&quot;]</span></code> otherwise.</p>
<p><strong>Remark</strong>: If you want to plot the training/validation loss curves using the <code class="docutils literal notranslate"><span class="pre">Trainer.plot_loss</span></code> method for your own model base, you should record the losses as lists in <code class="docutils literal notranslate"><span class="pre">self.train_losses</span></code>, <code class="docutils literal notranslate"><span class="pre">self.val_losses</span></code>, and <code class="docutils literal notranslate"><span class="pre">self.earlystopping_epoch</span></code> after training in <code class="docutils literal notranslate"><span class="pre">_train_single_model</span></code>. See source codes of <code class="docutils literal notranslate"><span class="pre">PytorchTabular</span></code>, <code class="docutils literal notranslate"><span class="pre">WideDeep</span></code>, or <code class="docutils literal notranslate"><span class="pre">TorchModel</span></code> for details.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_train_single_model</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">model_name</span><span class="p">,</span>
    <span class="n">epoch</span><span class="p">,</span>
    <span class="n">X_train</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">,</span>
    <span class="n">X_val</span><span class="p">,</span>
    <span class="n">y_val</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">,</span>
    <span class="n">warm_start</span><span class="p">,</span>
    <span class="n">in_bayes_opt</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">eval_set</span> <span class="o">=</span> <span class="p">[(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task</span> <span class="o">==</span> <span class="s2">&quot;regression&quot;</span> <span class="k">else</span> <span class="n">y_val</span><span class="o">.</span><span class="n">flatten</span><span class="p">())]</span>

    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
        <span class="n">X_train</span><span class="p">,</span>
        <span class="n">y_train</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task</span> <span class="o">==</span> <span class="s2">&quot;regression&quot;</span> <span class="k">else</span> <span class="n">y_train</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
        <span class="n">eval_set</span><span class="o">=</span><span class="n">eval_set</span><span class="p">,</span>
        <span class="n">max_epochs</span><span class="o">=</span><span class="n">epoch</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">in_bayes_opt</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;bayes_epoch&quot;</span><span class="p">],</span>
        <span class="n">patience</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;patience&quot;</span><span class="p">],</span>
        <span class="n">loss_fn</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task</span> <span class="o">==</span> <span class="s2">&quot;regression&quot;</span>
        <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(),</span>
        <span class="n">eval_metric</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mse&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task</span> <span class="o">==</span> <span class="s2">&quot;regression&quot;</span> <span class="k">else</span> <span class="s2">&quot;logloss&quot;</span><span class="p">],</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">]),</span>
        <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
        <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>To evaluate the model or make use of the model, <code class="docutils literal notranslate"><span class="pre">_pred_single_model</span></code> is defined, and <code class="docutils literal notranslate"><span class="pre">X_test</span></code> processed in <code class="docutils literal notranslate"><span class="pre">_train_data_preprocess</span></code> or <code class="docutils literal notranslate"><span class="pre">_data_preprocess</span></code> is passed as an argument. The returned value should always be a two-dimensional <code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code>. For binary classification tasks, the output is the probability of the positive (1) class, and for multiclass classification, the output is the probability of each class. <code class="docutils literal notranslate"><span class="pre">AbstractModel</span></code> automatically deals with the probabilities for metrics
and final outputs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_pred_single_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">verbose</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task</span> <span class="o">==</span> <span class="s2">&quot;regression&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">task</span> <span class="o">==</span> <span class="s2">&quot;binary&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
<p>The full code is as follows:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TabNetFromAbstract</span><span class="p">(</span><span class="n">AbstractModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">some_param</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TabNetFromAbstract</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="c1"># Do something else here</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">some_param</span> <span class="o">=</span> <span class="n">some_param</span>
        <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_params</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_program_name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;TabNetFromAbstract&quot;</span>

    <span class="k">def</span> <span class="nf">_get_model_names</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="s2">&quot;TabNet&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_space</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="n">Integer</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;n_d&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">),</span>
            <span class="n">Integer</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;n_a&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">),</span>
            <span class="n">Integer</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;n_steps&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">),</span>
            <span class="n">Real</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;gamma&quot;</span><span class="p">),</span>
            <span class="n">Integer</span><span class="p">(</span>
                <span class="n">low</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;n_independent&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span>
            <span class="p">),</span>
            <span class="n">Integer</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;n_shared&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">),</span>
        <span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">SPACE</span>

    <span class="k">def</span> <span class="nf">_initial_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;n_d&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
            <span class="s2">&quot;n_a&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
            <span class="s2">&quot;n_steps&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
            <span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="mf">1.3</span><span class="p">,</span>
            <span class="s2">&quot;n_independent&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
            <span class="s2">&quot;n_shared&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
            <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">],</span>
            <span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;weight_decay&quot;</span><span class="p">],</span>
            <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">],</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">_train_data_preprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">datamodule</span>
        <span class="n">all_feature_names</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">all_feature_names</span>

        <span class="n">X_train</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data_transform</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">X_train</span><span class="p">,</span> <span class="n">scaler_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span>
            <span class="n">all_feature_names</span>
        <span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">X_val</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data_transform</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">X_val</span><span class="p">,</span> <span class="n">scaler_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span>
            <span class="n">all_feature_names</span>
        <span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">X_test</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data_transform</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">X_test</span><span class="p">,</span> <span class="n">scaler_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span>
            <span class="n">all_feature_names</span>
        <span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">y_train</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">y_train</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">y_val</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">y_val</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">y_test</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">y_test</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;X_train&quot;</span><span class="p">:</span> <span class="n">X_train</span><span class="p">,</span>
            <span class="s2">&quot;y_train&quot;</span><span class="p">:</span> <span class="n">y_train</span><span class="p">,</span>
            <span class="s2">&quot;X_val&quot;</span><span class="p">:</span> <span class="n">X_val</span><span class="p">,</span>
            <span class="s2">&quot;y_val&quot;</span><span class="p">:</span> <span class="n">y_val</span><span class="p">,</span>
            <span class="s2">&quot;X_test&quot;</span><span class="p">:</span> <span class="n">X_test</span><span class="p">,</span>
            <span class="s2">&quot;y_test&quot;</span><span class="p">:</span> <span class="n">y_test</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">_data_preprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">derived_data</span><span class="p">,</span> <span class="n">model_name</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">datamodule</span><span class="o">.</span><span class="n">data_transform</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">scaler_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">all_feature_names</span>
        <span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_new_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">verbose</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">pytorch_tabnet.tab_model</span> <span class="kn">import</span> <span class="n">TabNetRegressor</span><span class="p">,</span> <span class="n">TabNetClassifier</span>

        <span class="n">datamodule</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">datamodule</span>
        <span class="n">cat_idxs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">datamodule</span><span class="o">.</span><span class="n">cont_feature_names</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">datamodule</span><span class="o">.</span><span class="n">all_feature_names</span><span class="p">)))</span>
        <span class="n">cat_dims</span> <span class="o">=</span> <span class="n">datamodule</span><span class="o">.</span><span class="n">cat_num_unique</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">task</span> <span class="o">=</span> <span class="n">datamodule</span><span class="o">.</span><span class="n">task</span>
        <span class="n">init_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">tabensemb</span><span class="o">.</span><span class="n">setting</span><span class="p">[</span><span class="s2">&quot;verbose_per_epoch&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="n">verbose</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
            <span class="n">optimizer_params</span><span class="o">=</span><span class="p">{</span>
                <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">],</span>
                <span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;weight_decay&quot;</span><span class="p">],</span>
            <span class="p">},</span>
            <span class="n">cat_idxs</span><span class="o">=</span><span class="n">cat_idxs</span><span class="p">,</span>
            <span class="n">cat_dims</span><span class="o">=</span><span class="n">cat_dims</span><span class="p">,</span>
            <span class="n">cat_emb_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
            <span class="n">device_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">datamodule</span><span class="o">.</span><span class="n">task</span> <span class="o">==</span> <span class="s2">&quot;regression&quot;</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">TabNetRegressor</span><span class="p">(</span><span class="o">**</span><span class="n">init_kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">TabNetClassifier</span><span class="p">(</span><span class="o">**</span><span class="n">init_kwargs</span><span class="p">)</span>

        <span class="n">model</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span>
            <span class="o">**</span><span class="p">{</span>
                <span class="s2">&quot;n_d&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;n_d&quot;</span><span class="p">],</span>
                <span class="s2">&quot;n_a&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;n_a&quot;</span><span class="p">],</span>
                <span class="s2">&quot;n_steps&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;n_steps&quot;</span><span class="p">],</span>
                <span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;gamma&quot;</span><span class="p">],</span>
                <span class="s2">&quot;n_independent&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;n_independent&quot;</span><span class="p">],</span>
                <span class="s2">&quot;n_shared&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;n_shared&quot;</span><span class="p">],</span>
            <span class="p">}</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">model</span>

    <span class="k">def</span> <span class="nf">_train_single_model</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">model_name</span><span class="p">,</span>
        <span class="n">epoch</span><span class="p">,</span>
        <span class="n">X_train</span><span class="p">,</span>
        <span class="n">y_train</span><span class="p">,</span>
        <span class="n">X_val</span><span class="p">,</span>
        <span class="n">y_val</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">,</span>
        <span class="n">warm_start</span><span class="p">,</span>
        <span class="n">in_bayes_opt</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">eval_set</span> <span class="o">=</span> <span class="p">[(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task</span> <span class="o">==</span> <span class="s2">&quot;regression&quot;</span> <span class="k">else</span> <span class="n">y_val</span><span class="o">.</span><span class="n">flatten</span><span class="p">())]</span>

        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
            <span class="n">X_train</span><span class="p">,</span>
            <span class="n">y_train</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task</span> <span class="o">==</span> <span class="s2">&quot;regression&quot;</span> <span class="k">else</span> <span class="n">y_train</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
            <span class="n">eval_set</span><span class="o">=</span><span class="n">eval_set</span><span class="p">,</span>
            <span class="n">max_epochs</span><span class="o">=</span><span class="n">epoch</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">in_bayes_opt</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;bayes_epoch&quot;</span><span class="p">],</span>
            <span class="n">patience</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;patience&quot;</span><span class="p">],</span>
            <span class="n">loss_fn</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task</span> <span class="o">==</span> <span class="s2">&quot;regression&quot;</span>
            <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(),</span>
            <span class="n">eval_metric</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mse&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task</span> <span class="o">==</span> <span class="s2">&quot;regression&quot;</span> <span class="k">else</span> <span class="s2">&quot;logloss&quot;</span><span class="p">],</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">]),</span>
            <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
            <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_pred_single_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">verbose</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task</span> <span class="o">==</span> <span class="s2">&quot;regression&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">task</span> <span class="o">==</span> <span class="s2">&quot;binary&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="Example:-Implement-TabNet-as-a-PyTorch-based-model">
<h2>Example: Implement TabNet as a <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code>-based model<a class="headerlink" href="#Example:-Implement-TabNet-as-a-PyTorch-based-model" title="Link to this heading">#</a></h2>
<p>Indeed, the example shown above uses <code class="docutils literal notranslate"><span class="pre">TabNetRegressor</span></code> and <code class="docutils literal notranslate"><span class="pre">TabNetClassifier</span></code> from <code class="docutils literal notranslate"><span class="pre">pytorch_tabnet</span></code> that have already implemented the training and evaluation procedures over the <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> subclass called <code class="docutils literal notranslate"><span class="pre">TabNet</span></code>. We can also directly build a model base for <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>s with less effort. These model bases inherit <code class="docutils literal notranslate"><span class="pre">TorchModel</span></code>, and <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>s should inherit <code class="docutils literal notranslate"><span class="pre">AbstractNN</span></code> (just needs to change a few lines to migrate previous code into this framework).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tabensemb.model</span> <span class="kn">import</span> <span class="n">TorchModel</span><span class="p">,</span> <span class="n">AbstractNN</span>
<span class="kn">from</span> <span class="nn">pytorch_tabnet.tab_network</span> <span class="kn">import</span> <span class="n">TabNet</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span>
</pre></div>
</div>
</div>
<p>First, we implement an <code class="docutils literal notranslate"><span class="pre">AbstractNN</span></code> (which inherits <code class="docutils literal notranslate"><span class="pre">pytorch_lightning.LightningModule</span></code> that further inherits <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>).</p>
<p>We initialize the model in <code class="docutils literal notranslate"><span class="pre">__init__</span></code>. <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> will depend on the arguments passed from <code class="docutils literal notranslate"><span class="pre">_new_model</span></code>, which will be implemented later, but at least it should contain all keys defined in <code class="docutils literal notranslate"><span class="pre">_initial_values</span></code>, as introduced in an above remark.</p>
<p>Remember to call <code class="docutils literal notranslate"><span class="pre">super().__init__</span></code>. There is nothing more difficult than initializing a <code class="docutils literal notranslate"><span class="pre">LightningModule</span></code>.</p>
<p>We can use <code class="docutils literal notranslate"><span class="pre">self.hparams.some_param</span></code> to get a hyperparameter (equivalent to <code class="docutils literal notranslate"><span class="pre">kwargs[&quot;some_param&quot;]</span></code>) if you call <code class="docutils literal notranslate"><span class="pre">super().__init__(datamodule,</span> <span class="pre">**kwargs)</span></code> instead of <code class="docutils literal notranslate"><span class="pre">super().__init__(datamodule)</span></code> because <code class="docutils literal notranslate"><span class="pre">AbstractNN</span></code> uses the <code class="docutils literal notranslate"><span class="pre">LightningModule.save_hyperparameters</span></code> utility (which you should <strong>not</strong> call in your own <code class="docutils literal notranslate"><span class="pre">__init__</span></code>).</p>
<p><strong>Remark</strong>: To migrate existing <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> code (Part 1)</p>
<ul class="simple">
<li><p>Change <code class="docutils literal notranslate"><span class="pre">class</span> <span class="pre">SomeModel(nn.Module)</span></code> to <code class="docutils literal notranslate"><span class="pre">class</span> <span class="pre">SomeModel(AbstractNN)</span></code>.</p></li>
<li><p>Change the indices of categorical features to <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">...,</span> <span class="pre">self.n_cat-1]</span></code> and the numbers of unique categories of categorical features to <code class="docutils literal notranslate"><span class="pre">self.cat_num_unique</span></code>.</p></li>
<li><p>Change the number of input dimensions to <code class="docutils literal notranslate"><span class="pre">self.n_cont+self.n_cat</span></code> and the number of output dimensions <code class="docutils literal notranslate"><span class="pre">self.n_outputs</span></code>.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TabNetNN</span><span class="p">(</span><span class="n">AbstractNN</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">datamodule</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TabNetNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">datamodule</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span> <span class="o">=</span> <span class="n">TabNet</span><span class="p">(</span>
            <span class="n">input_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span><span class="p">,</span>
            <span class="n">output_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">,</span>
            <span class="n">n_d</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">n_d</span><span class="p">,</span>
            <span class="n">n_a</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">n_a</span><span class="p">,</span>
            <span class="n">n_steps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">n_steps</span><span class="p">,</span>
            <span class="n">gamma</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span>
            <span class="n">cat_idxs</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span><span class="p">)),</span>
            <span class="n">cat_dims</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cat_num_unique</span><span class="p">,</span>
            <span class="n">cat_emb_dim</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span><span class="p">,</span>
            <span class="n">n_independent</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">n_independent</span><span class="p">,</span>
            <span class="n">n_shared</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">n_shared</span><span class="p">,</span>
        <span class="p">)</span>
</pre></div>
</div>
<p>Then we implement the computation step of the model. We should implement <code class="docutils literal notranslate"><span class="pre">_forward</span></code> instead of <code class="docutils literal notranslate"><span class="pre">forward</span></code> which is already implemented by <code class="docutils literal notranslate"><span class="pre">AbstractNN</span></code> and is used to automatically process inputs and outputs of <code class="docutils literal notranslate"><span class="pre">_forward</span></code>.</p>
<p>There are two input arguments for <code class="docutils literal notranslate"><span class="pre">_forward</span></code>: <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">derived_tensors</span></code>. <code class="docutils literal notranslate"><span class="pre">x</span></code> is a tensor of continuous features. <code class="docutils literal notranslate"><span class="pre">derived_tensors</span></code> is a dictionary containing contents in <code class="docutils literal notranslate"><span class="pre">datamodule.derived_data</span></code> (which is introduced in the last two sections of the “Using data functionalities” part), including categorical data (with the key “categorical” if there is any categorical feature), the signal for each data point representing whether it is an augmented one (with the key “augmented” if there
is any augmented data point), and derived unstacked data (with the key <code class="docutils literal notranslate"><span class="pre">derived_name</span></code> specified in the configuration). This is how multimodal data is passed to a deep learning model in our framework.</p>
<p>In the following lines, we build the input of the neural network from the continuous features <code class="docutils literal notranslate"><span class="pre">x</span></code> and the categorical features <code class="docutils literal notranslate"><span class="pre">derived_tensors[&quot;categorical&quot;]</span></code> by concatenation (that’s why the indices of categorical features are set to <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">...,</span> <span class="pre">self.n_cat-1]</span></code>), calculate the output of the network, and return the output.</p>
<p><strong>Remark</strong>: The default loss function is <code class="docutils literal notranslate"><span class="pre">torch.nn.MSELoss</span></code> for regression, <code class="docutils literal notranslate"><span class="pre">torch.nn.BCEWithLogitsLoss</span></code> for binary classification, and <code class="docutils literal notranslate"><span class="pre">torch.nn.CrossEntropyLoss</span></code> for multiclass classification. To change this behavior, implement <code class="docutils literal notranslate"><span class="pre">self.loss_fn</span></code>. See the “Advanced customized model base” part for details.</p>
<p><strong>Remark</strong>: For binary classification tasks, <code class="docutils literal notranslate"><span class="pre">self.n_outputs=1</span></code> so we expect the logits of the positive class (instead of a normalized probability). The output is then used to calculate <code class="docutils literal notranslate"><span class="pre">torch.nn.BCEWithLogitsLoss</span></code> by default. For multiclass classification tasks, <code class="docutils literal notranslate"><span class="pre">self.n_outputs</span></code> is the number of classes, so we expect the logits of these classes (instead of probabilities from <code class="docutils literal notranslate"><span class="pre">Softmax</span></code> or something else). The output is then used to calculate <code class="docutils literal notranslate"><span class="pre">torch.nn.CrossEntropyLoss</span></code> by default.</p>
<p><strong>Remark</strong>: To migrate existing <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> code (Part 2)</p>
<ul class="simple">
<li><p>Change <code class="docutils literal notranslate"><span class="pre">forward</span></code> to <code class="docutils literal notranslate"><span class="pre">_forward</span></code></p></li>
<li><p>Get categorical features from <code class="docutils literal notranslate"><span class="pre">derived_tensors</span></code></p></li>
<li><p>Get multimodal features from <code class="docutils literal notranslate"><span class="pre">derived_tensors</span></code> (and load multimodal features using data derivers)</p></li>
<li><p>Return logits instead of probabilities</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">derived_tensors</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">x_cont</span> <span class="o">=</span> <span class="n">x</span>
    <span class="k">if</span> <span class="s2">&quot;categorical&quot;</span> <span class="ow">in</span> <span class="n">derived_tensors</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">x_cat</span> <span class="o">=</span> <span class="n">derived_tensors</span><span class="p">[</span><span class="s2">&quot;categorical&quot;</span><span class="p">]</span>
        <span class="n">x_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">x_cat</span><span class="p">,</span> <span class="n">x_cont</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x_in</span> <span class="o">=</span> <span class="n">x_cont</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">(</span><span class="n">x_in</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
<p>The code is as follows:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TabNetNN</span><span class="p">(</span><span class="n">AbstractNN</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">datamodule</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TabNetNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">datamodule</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span> <span class="o">=</span> <span class="n">TabNet</span><span class="p">(</span>
            <span class="n">input_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span><span class="p">,</span>
            <span class="n">output_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">,</span>
            <span class="n">n_d</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">n_d</span><span class="p">,</span>
            <span class="n">n_a</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">n_a</span><span class="p">,</span>
            <span class="n">n_steps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">n_steps</span><span class="p">,</span>
            <span class="n">gamma</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span>
            <span class="n">cat_idxs</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span><span class="p">)),</span>
            <span class="n">cat_dims</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cat_num_unique</span><span class="p">,</span>
            <span class="n">cat_emb_dim</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span><span class="p">,</span>
            <span class="n">n_independent</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">n_independent</span><span class="p">,</span>
            <span class="n">n_shared</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">n_shared</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">derived_tensors</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">x_cont</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">if</span> <span class="s2">&quot;categorical&quot;</span> <span class="ow">in</span> <span class="n">derived_tensors</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">x_cat</span> <span class="o">=</span> <span class="n">derived_tensors</span><span class="p">[</span><span class="s2">&quot;categorical&quot;</span><span class="p">]</span>
            <span class="n">x_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">x_cat</span><span class="p">,</span> <span class="n">x_cont</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x_in</span> <span class="o">=</span> <span class="n">x_cont</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">(</span><span class="n">x_in</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
</div>
<p>Finally, we build the model base for the neural network. It inherits <code class="docutils literal notranslate"><span class="pre">TorchModel</span></code> which has implemented most required methods. Necessary methods for <code class="docutils literal notranslate"><span class="pre">TorchModel</span></code> can be written similarly with <code class="docutils literal notranslate"><span class="pre">TabNetFromAbstract</span></code>.</p>
<p><strong>Remark</strong>: PyTorch-based models are trained using <code class="docutils literal notranslate"><span class="pre">pytorch_lightning.Trainer</span></code>, whose arguments can be specified by passing them to <code class="docutils literal notranslate"><span class="pre">TorchModel.__init__</span></code> as a dictionary using the key <code class="docutils literal notranslate"><span class="pre">lightning_trainer_kwargs</span></code>.</p>
<p>In the following implementation, <code class="docutils literal notranslate"><span class="pre">_new_model</span></code> passes the datamodule and hyperparameters to the neural network, which is what you saw above in <code class="docutils literal notranslate"><span class="pre">__init__</span></code>. You can also pass other arguments as you want.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TabNetFromTorch</span><span class="p">(</span><span class="n">TorchModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_new_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">verbose</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">TabNetNN</span><span class="p">(</span><span class="n">datamodule</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">datamodule</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_program_name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;TabNetFromTorch&quot;</span>

    <span class="k">def</span> <span class="nf">_get_model_names</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="s2">&quot;TabNet&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_space</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="n">Integer</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;n_d&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">),</span>
            <span class="n">Integer</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;n_a&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">),</span>
            <span class="n">Integer</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;n_steps&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">),</span>
            <span class="n">Real</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;gamma&quot;</span><span class="p">),</span>
            <span class="n">Integer</span><span class="p">(</span>
                <span class="n">low</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;n_independent&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span>
            <span class="p">),</span>
            <span class="n">Integer</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;n_shared&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">),</span>
        <span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">SPACE</span>

    <span class="k">def</span> <span class="nf">_initial_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;n_d&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
            <span class="s2">&quot;n_a&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
            <span class="s2">&quot;n_steps&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
            <span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="mf">1.3</span><span class="p">,</span>
            <span class="s2">&quot;n_independent&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
            <span class="s2">&quot;n_shared&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
            <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">],</span>
            <span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;weight_decay&quot;</span><span class="p">],</span>
            <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">],</span>
        <span class="p">}</span>
</pre></div>
</div>
</div>
</section>
<section id="Comparison-of-different-implementations-in-other-model-bases">
<h2>Comparison of different implementations in other model bases<a class="headerlink" href="#Comparison-of-different-implementations-in-other-model-bases" title="Link to this heading">#</a></h2>
<p>We can compare our models with TabNet implemented in the other two model bases. Note that because of different training routines and randomization, they perform differently. Let’s try the models on a regression task first.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tabensemb.trainer</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">tabensemb.config</span> <span class="kn">import</span> <span class="n">UserConfig</span>
<span class="kn">from</span> <span class="nn">tabensemb.model</span> <span class="kn">import</span> <span class="n">PytorchTabular</span><span class="p">,</span> <span class="n">WideDeep</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">mpg_columns</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;mpg&quot;</span><span class="p">,</span>
    <span class="s2">&quot;cylinders&quot;</span><span class="p">,</span>
    <span class="s2">&quot;displacement&quot;</span><span class="p">,</span>
    <span class="s2">&quot;horsepower&quot;</span><span class="p">,</span>
    <span class="s2">&quot;weight&quot;</span><span class="p">,</span>
    <span class="s2">&quot;acceleration&quot;</span><span class="p">,</span>
    <span class="s2">&quot;model_year&quot;</span><span class="p">,</span>
    <span class="s2">&quot;origin&quot;</span><span class="p">,</span>
    <span class="s2">&quot;car_name&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">cfg</span> <span class="o">=</span> <span class="n">UserConfig</span><span class="o">.</span><span class="n">from_uci</span><span class="p">(</span><span class="s2">&quot;Auto MPG&quot;</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="n">mpg_columns</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;\s+&quot;</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">load_config</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">add_modelbases</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">PytorchTabular</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">model_subset</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;TabNet&quot;</span><span class="p">]),</span>
        <span class="n">WideDeep</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">model_subset</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;TabNet&quot;</span><span class="p">]),</span>
        <span class="n">TabNetFromAbstract</span><span class="p">(</span><span class="n">trainer</span><span class="p">),</span>
        <span class="n">TabNetFromTorch</span><span class="p">(</span><span class="n">trainer</span><span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">stderr_to_stdout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">get_leaderboard</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Downloading https://archive.ics.uci.edu/static/public/9/auto+mpg.zip to /tmp/tmpdriivjp7/data/Auto MPG.zip
cylinders is Integer and will be treated as a continuous feature.
model_year is Integer and will be treated as a continuous feature.
origin is Integer and will be treated as a continuous feature.
Unknown values are detected in [&#39;horsepower&#39;]. They will be treated as np.nan.
The project will be saved to /tmp/tmpdriivjp7/output/auto-mpg/2023-09-23-20-34-51-0_UserInputConfig
Dataset size: 238 80 80
Data saved to /tmp/tmpdriivjp7/output/auto-mpg/2023-09-23-20-34-51-0_UserInputConfig (data.csv and tabular_data.csv).
{&#39;some_param&#39;: 1.1, &#39;program&#39;: None, &#39;model_subset&#39;: None, &#39;exclude_models&#39;: None, &#39;store_in_harddisk&#39;: True}

-------------Run PytorchTabular-------------

Training TabNet
Global seed set to 42
2023-09-23 20:34:52,661 - {pytorch_tabular.tabular_model:473} - INFO - Preparing the DataLoaders
2023-09-23 20:34:52,661 - {pytorch_tabular.tabular_datamodule:290} - INFO - Setting up the datamodule for regression task
2023-09-23 20:34:52,670 - {pytorch_tabular.tabular_model:521} - INFO - Preparing the Model: TabNetModel
2023-09-23 20:34:52,684 - {pytorch_tabular.tabular_model:268} - INFO - Preparing the Trainer
/home/xlluo/anaconda3/envs/tabular_ensemble/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:589: LightningDeprecationWarning: The Trainer argument `auto_select_gpus` has been deprecated in v1.9.0 and will be removed in v2.0.0. Please use the function `pytorch_lightning.accelerators.find_usable_cuda_devices` instead.
  rank_zero_deprecation(
Auto select gpus: [0]
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2023-09-23 20:34:53,558 - {pytorch_tabular.tabular_model:582} - INFO - Training Started
You are using a CUDA device (&#39;NVIDIA GeForce RTX 3090&#39;) that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision(&#39;medium&#39; | &#39;high&#39;)` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name             | Type           | Params
----------------------------------------------------
0 | _embedding_layer | Identity       | 0
1 | _backbone        | TabNetBackbone | 6.1 K
2 | _head            | Identity       | 0
3 | loss             | MSELoss        | 0
----------------------------------------------------
6.1 K     Trainable params
0         Non-trainable params
6.1 K     Total params
0.024     Total estimated model params size (MB)
Epoch: 1/300, Train loss: 631.3760, Val loss: 566.2643, Min val loss: 566.2643, Epoch time: 0.029s.
Epoch: 20/300, Train loss: 601.5834, Val loss: 530.7571, Min val loss: 530.7571, Epoch time: 0.023s.
Epoch: 40/300, Train loss: 573.0287, Val loss: 512.0469, Min val loss: 512.0469, Epoch time: 0.024s.
Epoch: 60/300, Train loss: 547.0676, Val loss: 488.9417, Min val loss: 488.9417, Epoch time: 0.026s.
Epoch: 80/300, Train loss: 521.3848, Val loss: 460.8999, Min val loss: 460.8999, Epoch time: 0.024s.
Epoch: 100/300, Train loss: 492.7377, Val loss: 429.2286, Min val loss: 429.2286, Epoch time: 0.023s.
Epoch: 120/300, Train loss: 461.0415, Val loss: 398.5600, Min val loss: 398.5600, Epoch time: 0.025s.
Epoch: 140/300, Train loss: 430.3234, Val loss: 374.5228, Min val loss: 374.5228, Epoch time: 0.035s.
Epoch: 160/300, Train loss: 397.2393, Val loss: 348.5988, Min val loss: 348.5988, Epoch time: 0.028s.
Epoch: 180/300, Train loss: 370.6253, Val loss: 322.0574, Min val loss: 322.0574, Epoch time: 0.026s.
Epoch: 200/300, Train loss: 340.3246, Val loss: 301.0881, Min val loss: 301.0881, Epoch time: 0.028s.
Epoch: 220/300, Train loss: 315.3022, Val loss: 277.9825, Min val loss: 277.9825, Epoch time: 0.024s.
Epoch: 240/300, Train loss: 287.3188, Val loss: 257.2012, Min val loss: 257.2012, Epoch time: 0.024s.
Epoch: 260/300, Train loss: 260.1859, Val loss: 233.5729, Min val loss: 233.5729, Epoch time: 0.024s.
Epoch: 280/300, Train loss: 235.5418, Val loss: 206.5197, Min val loss: 206.5197, Epoch time: 0.025s.
Epoch: 300/300, Train loss: 211.4229, Val loss: 186.0890, Min val loss: 186.0890, Epoch time: 0.046s.
`Trainer.fit` stopped: `max_epochs=300` reached.
2023-09-23 20:35:05,998 - {pytorch_tabular.tabular_model:584} - INFO - Training the model completed
2023-09-23 20:35:05,998 - {pytorch_tabular.tabular_model:1258} - INFO - Loading the best model
/home/xlluo/anaconda3/envs/tabular_ensemble/lib/python3.10/site-packages/pytorch_lightning/utilities/cloud_io.py:33: LightningDeprecationWarning: `pytorch_lightning.utilities.cloud_io.get_filesystem` has been deprecated in v1.8.0 and will be removed in v2.0.0. Please use `lightning_fabric.utilities.cloud_io.get_filesystem` instead.
  rank_zero_deprecation(
Training mse loss: 204.72249
Validation mse loss: 186.08899
Testing mse loss: 211.05500
Trainer saved. To load the trainer, run trainer = load_trainer(path=&#39;/tmp/tmpdriivjp7/output/auto-mpg/2023-09-23-20-34-51-0_UserInputConfig/trainer.pkl&#39;)

-------------PytorchTabular End-------------


-------------Run WideDeep-------------

Training TabNet
Epoch: 1/300, Train loss: 632.3985, Val loss: 567.4960, Min val loss: 567.4960
Epoch: 21/300, Train loss: 598.0990, Val loss: 533.1615, Min val loss: 533.1615
Epoch: 41/300, Train loss: 569.2331, Val loss: 512.1868, Min val loss: 512.1868
Epoch: 61/300, Train loss: 540.4758, Val loss: 484.7060, Min val loss: 484.7060
Epoch: 81/300, Train loss: 511.0865, Val loss: 455.2033, Min val loss: 455.2033
Epoch: 101/300, Train loss: 480.2253, Val loss: 424.2426, Min val loss: 424.2426
Epoch: 121/300, Train loss: 450.1095, Val loss: 398.7469, Min val loss: 398.7469
Epoch: 141/300, Train loss: 419.1121, Val loss: 373.4113, Min val loss: 373.4113
Epoch: 161/300, Train loss: 389.0500, Val loss: 343.9605, Min val loss: 343.9605
Epoch: 181/300, Train loss: 359.7761, Val loss: 317.2437, Min val loss: 317.2437
Epoch: 201/300, Train loss: 332.5761, Val loss: 289.8560, Min val loss: 289.8560
Epoch: 221/300, Train loss: 304.8683, Val loss: 268.5120, Min val loss: 268.5120
Epoch: 241/300, Train loss: 278.2647, Val loss: 245.0433, Min val loss: 245.0433
Epoch: 261/300, Train loss: 252.8031, Val loss: 220.4438, Min val loss: 220.4438
Epoch: 281/300, Train loss: 228.1485, Val loss: 196.3897, Min val loss: 196.3897
Restoring model weights from the end of the best epoch
Training mse loss: 206.63809
Validation mse loss: 173.36779
Testing mse loss: 212.69775
Trainer saved. To load the trainer, run trainer = load_trainer(path=&#39;/tmp/tmpdriivjp7/output/auto-mpg/2023-09-23-20-34-51-0_UserInputConfig/trainer.pkl&#39;)

-------------WideDeep End-------------


-------------Run TabNetFromAbstract-------------

Training TabNet
/home/xlluo/anaconda3/envs/tabular_ensemble/lib/python3.10/site-packages/pytorch_tabnet/multiclass_utils.py:13: DeprecationWarning: Please use `spmatrix` from the `scipy.sparse` namespace, the `scipy.sparse.base` namespace is deprecated.
  from scipy.sparse.base import spmatrix
/home/xlluo/anaconda3/envs/tabular_ensemble/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda
  warnings.warn(f&#34;Device used : {self.device}&#34;)
epoch 0  | loss: 587.08069| val_0_mse: 521.36511|  0:00:00s
epoch 20 | loss: 543.03503| val_0_mse: 509.49057|  0:00:00s
epoch 40 | loss: 503.94666| val_0_mse: 474.56982|  0:00:01s
epoch 60 | loss: 464.24173| val_0_mse: 433.25231|  0:00:01s
epoch 80 | loss: 428.79398| val_0_mse: 394.9299|  0:00:02s
epoch 100| loss: 398.57919| val_0_mse: 363.43954|  0:00:02s
epoch 120| loss: 368.87878| val_0_mse: 332.14944|  0:00:03s
epoch 140| loss: 339.44043| val_0_mse: 301.91438|  0:00:03s
epoch 160| loss: 313.07431| val_0_mse: 274.16858|  0:00:04s
epoch 180| loss: 287.4639| val_0_mse: 248.59148|  0:00:04s
epoch 200| loss: 255.01363| val_0_mse: 223.61368|  0:00:05s
epoch 220| loss: 229.25694| val_0_mse: 198.16758|  0:00:06s
epoch 240| loss: 200.16383| val_0_mse: 172.72145|  0:00:06s
epoch 260| loss: 176.21191| val_0_mse: 150.20621|  0:00:07s
epoch 280| loss: 150.32635| val_0_mse: 132.21739|  0:00:07s
Stop training because you reached max_epochs = 300 with best_epoch = 299 and best_val_0_mse = 110.85251
/home/xlluo/anaconda3/envs/tabular_ensemble/lib/python3.10/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
Training mse loss: 122.27976
Validation mse loss: 110.85251
Testing mse loss: 112.14110
Trainer saved. To load the trainer, run trainer = load_trainer(path=&#39;/tmp/tmpdriivjp7/output/auto-mpg/2023-09-23-20-34-51-0_UserInputConfig/trainer.pkl&#39;)

-------------TabNetFromAbstract End-------------


-------------Run TabNetFromTorch-------------

Training TabNet
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device (&#39;NVIDIA GeForce RTX 3090&#39;) that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision(&#39;medium&#39; | &#39;high&#39;)` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name                | Type     | Params
-------------------------------------------------
0 | default_loss_fn     | MSELoss  | 0
1 | default_output_norm | Identity | 0
2 | network             | TabNet   | 6.1 K
-------------------------------------------------
6.1 K     Trainable params
0         Non-trainable params
6.1 K     Total params
0.024     Total estimated model params size (MB)
Epoch: 1/300, Train loss: 631.3761, Val loss: 568.1518, Min val loss: 568.1518, Min ES val loss: 568.1518, Epoch time: 0.021s.
Epoch: 20/300, Train loss: 601.5826, Val loss: 531.1477, Min val loss: 531.1477, Min ES val loss: 531.1477, Epoch time: 0.020s.
Epoch: 40/300, Train loss: 572.3008, Val loss: 512.4727, Min val loss: 512.4727, Min ES val loss: 512.4727, Epoch time: 0.020s.
Epoch: 60/300, Train loss: 547.1713, Val loss: 487.6359, Min val loss: 487.6359, Min ES val loss: 487.6359, Epoch time: 0.021s.
Epoch: 80/300, Train loss: 521.3299, Val loss: 458.8514, Min val loss: 458.8514, Min ES val loss: 458.8514, Epoch time: 0.023s.
Epoch: 100/300, Train loss: 493.5303, Val loss: 430.2360, Min val loss: 430.2360, Min ES val loss: 430.2360, Epoch time: 0.024s.
Epoch: 120/300, Train loss: 460.1804, Val loss: 404.0699, Min val loss: 404.0699, Min ES val loss: 404.0699, Epoch time: 0.022s.
Epoch: 140/300, Train loss: 430.0944, Val loss: 377.3679, Min val loss: 377.3679, Min ES val loss: 377.3679, Epoch time: 0.022s.
Epoch: 160/300, Train loss: 399.9077, Val loss: 349.6179, Min val loss: 349.6179, Min ES val loss: 349.6179, Epoch time: 0.026s.
Epoch: 180/300, Train loss: 373.4857, Val loss: 330.2073, Min val loss: 330.2073, Min ES val loss: 330.2073, Epoch time: 0.022s.
Epoch: 200/300, Train loss: 342.6961, Val loss: 307.0838, Min val loss: 307.0838, Min ES val loss: 307.0838, Epoch time: 0.024s.
Epoch: 220/300, Train loss: 318.1428, Val loss: 283.2042, Min val loss: 283.2042, Min ES val loss: 283.2042, Epoch time: 0.023s.
Epoch: 240/300, Train loss: 292.0168, Val loss: 260.7692, Min val loss: 260.7692, Min ES val loss: 260.7692, Epoch time: 0.035s.
Epoch: 260/300, Train loss: 263.9176, Val loss: 236.7290, Min val loss: 236.7290, Min ES val loss: 236.7290, Epoch time: 0.037s.
Epoch: 280/300, Train loss: 241.0646, Val loss: 214.5846, Min val loss: 214.5846, Min ES val loss: 214.5846, Epoch time: 0.024s.
Epoch: 300/300, Train loss: 216.0625, Val loss: 191.7081, Min val loss: 191.7081, Min ES val loss: 191.7081, Epoch time: 0.031s.
`Trainer.fit` stopped: `max_epochs=300` reached.
Training mse loss: 208.48247
Validation mse loss: 191.70814
Testing mse loss: 204.47160
Trainer saved. To load the trainer, run trainer = load_trainer(path=&#39;/tmp/tmpdriivjp7/output/auto-mpg/2023-09-23-20-34-51-0_UserInputConfig/trainer.pkl&#39;)

-------------TabNetFromTorch End-------------

PytorchTabular metrics
TabNet 1/1
WideDeep metrics
TabNet 1/1
TabNetFromAbstract metrics
TabNet 1/1
TabNetFromTorch metrics
TabNet 1/1
Trainer saved. To load the trainer, run trainer = load_trainer(path=&#39;/tmp/tmpdriivjp7/output/auto-mpg/2023-09-23-20-34-51-0_UserInputConfig/trainer.pkl&#39;)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Program</th>
      <th>Model</th>
      <th>Training RMSE</th>
      <th>Training MSE</th>
      <th>Training MAE</th>
      <th>Training MAPE</th>
      <th>Training R2</th>
      <th>Training MEDIAN_ABSOLUTE_ERROR</th>
      <th>Training EXPLAINED_VARIANCE_SCORE</th>
      <th>Testing RMSE</th>
      <th>...</th>
      <th>Testing R2</th>
      <th>Testing MEDIAN_ABSOLUTE_ERROR</th>
      <th>Testing EXPLAINED_VARIANCE_SCORE</th>
      <th>Validation RMSE</th>
      <th>Validation MSE</th>
      <th>Validation MAE</th>
      <th>Validation MAPE</th>
      <th>Validation R2</th>
      <th>Validation MEDIAN_ABSOLUTE_ERROR</th>
      <th>Validation EXPLAINED_VARIANCE_SCORE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>TabNetFromAbstract</td>
      <td>TabNet</td>
      <td>11.058018</td>
      <td>122.279759</td>
      <td>10.120124</td>
      <td>0.425556</td>
      <td>-0.897031</td>
      <td>9.511139</td>
      <td>0.688216</td>
      <td>10.589669</td>
      <td>...</td>
      <td>-1.085708</td>
      <td>9.735932</td>
      <td>0.711417</td>
      <td>10.528652</td>
      <td>110.852511</td>
      <td>9.544754</td>
      <td>0.417929</td>
      <td>-0.980271</td>
      <td>9.028342</td>
      <td>0.575915</td>
    </tr>
    <tr>
      <th>1</th>
      <td>TabNetFromTorch</td>
      <td>TabNet</td>
      <td>14.438922</td>
      <td>208.482471</td>
      <td>13.648272</td>
      <td>0.578209</td>
      <td>-2.234367</td>
      <td>12.755719</td>
      <td>0.655482</td>
      <td>14.299357</td>
      <td>...</td>
      <td>-2.802959</td>
      <td>13.421765</td>
      <td>0.606030</td>
      <td>13.845871</td>
      <td>191.708137</td>
      <td>12.779896</td>
      <td>0.562416</td>
      <td>-2.424678</td>
      <td>11.474345</td>
      <td>0.490396</td>
    </tr>
    <tr>
      <th>2</th>
      <td>PytorchTabular</td>
      <td>TabNet</td>
      <td>14.308127</td>
      <td>204.722492</td>
      <td>13.486490</td>
      <td>0.570209</td>
      <td>-2.176035</td>
      <td>12.484611</td>
      <td>0.645709</td>
      <td>14.527732</td>
      <td>...</td>
      <td>-2.925404</td>
      <td>13.481362</td>
      <td>0.597145</td>
      <td>13.641444</td>
      <td>186.088988</td>
      <td>12.641969</td>
      <td>0.558155</td>
      <td>-2.324297</td>
      <td>11.443497</td>
      <td>0.530719</td>
    </tr>
    <tr>
      <th>3</th>
      <td>WideDeep</td>
      <td>TabNet</td>
      <td>14.374912</td>
      <td>206.638091</td>
      <td>13.423684</td>
      <td>0.562884</td>
      <td>-2.205754</td>
      <td>12.262027</td>
      <td>0.560759</td>
      <td>14.584161</td>
      <td>...</td>
      <td>-2.955957</td>
      <td>13.293868</td>
      <td>0.569623</td>
      <td>13.166920</td>
      <td>173.367793</td>
      <td>12.166225</td>
      <td>0.534651</td>
      <td>-2.097046</td>
      <td>11.464049</td>
      <td>0.411240</td>
    </tr>
  </tbody>
</table>
<p>4 rows × 23 columns</p>
</div></div>
</div>
<p>We can see that <code class="docutils literal notranslate"><span class="pre">TabNet</span></code> does not perform well with the current hyperparameters. We can use <code class="docutils literal notranslate"><span class="pre">trainer.args[&quot;bayes_opt&quot;]</span> <span class="pre">=</span> <span class="pre">True</span></code> to activate Bayesian hyperparameter optimization to improve its performance. Alternatively, we can directly provide a set of hyperparameters (which is found by Bayesian optimization) in <code class="docutils literal notranslate"><span class="pre">AbstractModel.model_params</span></code>. As shown below, the performance significantly improves.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">load_config</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="n">modelbase</span> <span class="o">=</span> <span class="n">PytorchTabular</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">model_subset</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;TabNet&quot;</span><span class="p">])</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">add_modelbases</span><span class="p">([</span><span class="n">modelbase</span><span class="p">])</span>
<span class="n">modelbase</span><span class="o">.</span><span class="n">model_params</span><span class="p">[</span><span class="s2">&quot;TabNet&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;n_d&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span> <span class="s1">&#39;n_a&#39;</span><span class="p">:</span> <span class="mi">15</span><span class="p">,</span> <span class="s1">&#39;n_steps&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;gamma&#39;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;n_independent&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">&#39;n_shared&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.026917811078469658</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">1e-09</span><span class="p">,</span> <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">64</span><span class="p">}</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">stderr_to_stdout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">get_leaderboard</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
The project will be saved to /tmp/tmpdriivjp7/output/auto-mpg/2023-09-23-20-35-35-0_UserInputConfig
Dataset size: 238 80 80
Data saved to /tmp/tmpdriivjp7/output/auto-mpg/2023-09-23-20-35-35-0_UserInputConfig (data.csv and tabular_data.csv).

-------------Run PytorchTabular-------------

Training TabNet
Previous params loaded: {&#39;n_d&#39;: 8, &#39;n_a&#39;: 15, &#39;n_steps&#39;: 1, &#39;gamma&#39;: 1.0, &#39;n_independent&#39;: 3, &#39;n_shared&#39;: 4, &#39;lr&#39;: 0.026917811078469658, &#39;weight_decay&#39;: 1e-09, &#39;batch_size&#39;: 64}
Global seed set to 42
2023-09-23 20:35:35,743 - {pytorch_tabular.tabular_model:473} - INFO - Preparing the DataLoaders
2023-09-23 20:35:35,744 - {pytorch_tabular.tabular_datamodule:290} - INFO - Setting up the datamodule for regression task
2023-09-23 20:35:35,754 - {pytorch_tabular.tabular_model:521} - INFO - Preparing the Model: TabNetModel
2023-09-23 20:35:35,766 - {pytorch_tabular.tabular_model:268} - INFO - Preparing the Trainer
/home/xlluo/anaconda3/envs/tabular_ensemble/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:589: LightningDeprecationWarning: The Trainer argument `auto_select_gpus` has been deprecated in v1.9.0 and will be removed in v2.0.0. Please use the function `pytorch_lightning.accelerators.find_usable_cuda_devices` instead.
  rank_zero_deprecation(
Auto select gpus: [0]
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2023-09-23 20:35:35,784 - {pytorch_tabular.tabular_model:582} - INFO - Training Started
You are using a CUDA device (&#39;NVIDIA GeForce RTX 3090&#39;) that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision(&#39;medium&#39; | &#39;high&#39;)` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name             | Type           | Params
----------------------------------------------------
0 | _embedding_layer | Identity       | 0
1 | _backbone        | TabNetBackbone | 11.3 K
2 | _head            | Identity       | 0
3 | loss             | MSELoss        | 0
----------------------------------------------------
11.3 K    Trainable params
0         Non-trainable params
11.3 K    Total params
0.045     Total estimated model params size (MB)
Epoch: 1/300, Train loss: 608.1827, Val loss: 442.9725, Min val loss: 442.9725, Epoch time: 0.066s.
Epoch: 20/300, Train loss: 8.9815, Val loss: 23.6918, Min val loss: 23.6918, Epoch time: 0.053s.
Epoch: 40/300, Train loss: 6.7731, Val loss: 13.2956, Min val loss: 9.6972, Epoch time: 0.061s.
Epoch: 60/300, Train loss: 6.0017, Val loss: 11.2620, Min val loss: 9.4719, Epoch time: 0.059s.
Epoch: 80/300, Train loss: 4.7520, Val loss: 10.6944, Min val loss: 9.4719, Epoch time: 0.082s.
Epoch: 100/300, Train loss: 6.2060, Val loss: 8.9962, Min val loss: 8.5929, Epoch time: 0.065s.
Epoch: 120/300, Train loss: 5.4083, Val loss: 11.0474, Min val loss: 8.5929, Epoch time: 0.056s.
Epoch: 140/300, Train loss: 4.6970, Val loss: 10.8864, Min val loss: 8.4415, Epoch time: 0.052s.
Epoch: 160/300, Train loss: 5.2870, Val loss: 10.9812, Min val loss: 8.4415, Epoch time: 0.053s.
Epoch: 180/300, Train loss: 4.4553, Val loss: 10.5833, Min val loss: 8.4415, Epoch time: 0.052s.
Epoch: 200/300, Train loss: 4.3323, Val loss: 12.8693, Min val loss: 8.4415, Epoch time: 0.058s.
Epoch: 220/300, Train loss: 4.1741, Val loss: 12.9377, Min val loss: 8.4415, Epoch time: 0.057s.
2023-09-23 20:35:49,765 - {pytorch_tabular.tabular_model:584} - INFO - Training the model completed
2023-09-23 20:35:49,766 - {pytorch_tabular.tabular_model:1258} - INFO - Loading the best model
/home/xlluo/anaconda3/envs/tabular_ensemble/lib/python3.10/site-packages/pytorch_lightning/utilities/cloud_io.py:33: LightningDeprecationWarning: `pytorch_lightning.utilities.cloud_io.get_filesystem` has been deprecated in v1.8.0 and will be removed in v2.0.0. Please use `lightning_fabric.utilities.cloud_io.get_filesystem` instead.
  rank_zero_deprecation(
Training mse loss: 4.88793
Validation mse loss: 8.44147
Testing mse loss: 6.77917
Trainer saved. To load the trainer, run trainer = load_trainer(path=&#39;/tmp/tmpdriivjp7/output/auto-mpg/2023-09-23-20-35-35-0_UserInputConfig/trainer.pkl&#39;)

-------------PytorchTabular End-------------

PytorchTabular metrics
TabNet 1/1
Trainer saved. To load the trainer, run trainer = load_trainer(path=&#39;/tmp/tmpdriivjp7/output/auto-mpg/2023-09-23-20-35-35-0_UserInputConfig/trainer.pkl&#39;)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Program</th>
      <th>Model</th>
      <th>Training RMSE</th>
      <th>Training MSE</th>
      <th>Training MAE</th>
      <th>Training MAPE</th>
      <th>Training R2</th>
      <th>Training MEDIAN_ABSOLUTE_ERROR</th>
      <th>Training EXPLAINED_VARIANCE_SCORE</th>
      <th>Testing RMSE</th>
      <th>...</th>
      <th>Testing R2</th>
      <th>Testing MEDIAN_ABSOLUTE_ERROR</th>
      <th>Testing EXPLAINED_VARIANCE_SCORE</th>
      <th>Validation RMSE</th>
      <th>Validation MSE</th>
      <th>Validation MAE</th>
      <th>Validation MAPE</th>
      <th>Validation R2</th>
      <th>Validation MEDIAN_ABSOLUTE_ERROR</th>
      <th>Validation EXPLAINED_VARIANCE_SCORE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>PytorchTabular</td>
      <td>TabNet</td>
      <td>2.210867</td>
      <td>4.887931</td>
      <td>1.642482</td>
      <td>0.069627</td>
      <td>0.924169</td>
      <td>1.215234</td>
      <td>0.924173</td>
      <td>2.603684</td>
      <td>...</td>
      <td>0.873915</td>
      <td>1.517266</td>
      <td>0.873926</td>
      <td>2.90542</td>
      <td>8.441465</td>
      <td>2.17954</td>
      <td>0.097734</td>
      <td>0.849201</td>
      <td>1.557569</td>
      <td>0.849222</td>
    </tr>
  </tbody>
</table>
<p>1 rows × 23 columns</p>
</div></div>
</div>
<p>Then the binary classification task:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">adult_columns</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;age&quot;</span><span class="p">,</span>
    <span class="s2">&quot;workclass&quot;</span><span class="p">,</span>
    <span class="s2">&quot;fnlwgt&quot;</span><span class="p">,</span>
    <span class="s2">&quot;education&quot;</span><span class="p">,</span>
    <span class="s2">&quot;education-num&quot;</span><span class="p">,</span>
    <span class="s2">&quot;marital-status&quot;</span><span class="p">,</span>
    <span class="s2">&quot;occupation&quot;</span><span class="p">,</span>
    <span class="s2">&quot;relationship&quot;</span><span class="p">,</span>
    <span class="s2">&quot;race&quot;</span><span class="p">,</span>
    <span class="s2">&quot;sex&quot;</span><span class="p">,</span>
    <span class="s2">&quot;capital-gain&quot;</span><span class="p">,</span>
    <span class="s2">&quot;capital-loss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;hours-per-week&quot;</span><span class="p">,</span>
    <span class="s2">&quot;native-country&quot;</span><span class="p">,</span>
    <span class="s2">&quot;income&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">cfg</span> <span class="o">=</span> <span class="n">UserConfig</span><span class="o">.</span><span class="n">from_uci</span><span class="p">(</span><span class="s2">&quot;Adult&quot;</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="n">adult_columns</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s2">&quot;, &quot;</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">load_config</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">add_modelbases</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">PytorchTabular</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">model_subset</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;TabNet&quot;</span><span class="p">]),</span>
        <span class="n">WideDeep</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">model_subset</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;TabNet&quot;</span><span class="p">]),</span>
        <span class="n">TabNetFromAbstract</span><span class="p">(</span><span class="n">trainer</span><span class="p">),</span>
        <span class="n">TabNetFromTorch</span><span class="p">(</span><span class="n">trainer</span><span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">stderr_to_stdout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">get_leaderboard</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Downloading https://archive.ics.uci.edu/static/public/2/adult.zip to /tmp/tmpdriivjp7/data/Adult.zip
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/home/xlluo/hdd/tabular_ensemble/tabensemb/config/user_config.py:292: UserWarning: There exists .test file(s) [&#39;adult.test&#39;] which should be used for final metrics. The .zip file is left for the user to process.
  warnings.warn(
/home/xlluo/hdd/tabular_ensemble/tabensemb/utils/utils.py:464: ParserWarning: Falling back to the &#39;python&#39; engine because the &#39;c&#39; engine does not support regex separators (separators &gt; 1 char and different from &#39;\s+&#39; are interpreted as regex); you can avoid this warning by specifying engine=&#39;python&#39;.
  df = pd.read_csv(StringIO(s), names=names, sep=sep)
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
age is Integer and will be treated as a continuous feature.
fnlwgt is Integer and will be treated as a continuous feature.
education-num is Integer and will be treated as a continuous feature.
capital-gain is Integer and will be treated as a continuous feature.
capital-loss is Integer and will be treated as a continuous feature.
hours-per-week is Integer and will be treated as a continuous feature.
The project will be saved to /tmp/tmpdriivjp7/output/adult/2023-09-23-20-35-54-0_UserInputConfig
Dataset size: 19536 6512 6513
Data saved to /tmp/tmpdriivjp7/output/adult/2023-09-23-20-35-54-0_UserInputConfig (data.csv and tabular_data.csv).
{&#39;some_param&#39;: 1.1, &#39;program&#39;: None, &#39;model_subset&#39;: None, &#39;exclude_models&#39;: None, &#39;store_in_harddisk&#39;: True}

-------------Run PytorchTabular-------------

Training TabNet
Global seed set to 42
2023-09-23 20:35:56,645 - {pytorch_tabular.tabular_model:473} - INFO - Preparing the DataLoaders
2023-09-23 20:35:56,648 - {pytorch_tabular.tabular_datamodule:290} - INFO - Setting up the datamodule for classification task
2023-09-23 20:35:56,719 - {pytorch_tabular.tabular_model:521} - INFO - Preparing the Model: TabNetModel
2023-09-23 20:35:56,741 - {pytorch_tabular.tabular_model:268} - INFO - Preparing the Trainer
/home/xlluo/anaconda3/envs/tabular_ensemble/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:589: LightningDeprecationWarning: The Trainer argument `auto_select_gpus` has been deprecated in v1.9.0 and will be removed in v2.0.0. Please use the function `pytorch_lightning.accelerators.find_usable_cuda_devices` instead.
  rank_zero_deprecation(
Auto select gpus: [0]
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2023-09-23 20:35:56,759 - {pytorch_tabular.tabular_model:582} - INFO - Training Started
You are using a CUDA device (&#39;NVIDIA GeForce RTX 3090&#39;) that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision(&#39;medium&#39; | &#39;high&#39;)` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name             | Type             | Params
------------------------------------------------------
0 | _embedding_layer | Identity         | 0
1 | _backbone        | TabNetBackbone   | 11.2 K
2 | _head            | Identity         | 0
3 | loss             | CrossEntropyLoss | 0
------------------------------------------------------
11.2 K    Trainable params
0         Non-trainable params
11.2 K    Total params
0.045     Total estimated model params size (MB)
Epoch: 1/300, Train loss: 1.1484, Val loss: 0.7944, Min val loss: 0.7944, Epoch time: 1.112s.
Epoch: 20/300, Train loss: 0.4990, Val loss: 0.4745, Min val loss: 0.4745, Epoch time: 0.782s.
Epoch: 40/300, Train loss: 0.4284, Val loss: 0.4167, Min val loss: 0.4167, Epoch time: 0.783s.
Epoch: 60/300, Train loss: 0.4016, Val loss: 0.3994, Min val loss: 0.3978, Epoch time: 0.880s.
Epoch: 80/300, Train loss: 0.3866, Val loss: 0.3913, Min val loss: 0.3896, Epoch time: 0.786s.
Epoch: 100/300, Train loss: 0.3802, Val loss: 0.3806, Min val loss: 0.3806, Epoch time: 0.912s.
Epoch: 120/300, Train loss: 0.3637, Val loss: 0.3661, Min val loss: 0.3661, Epoch time: 1.019s.
Epoch: 140/300, Train loss: 0.3539, Val loss: 0.3600, Min val loss: 0.3600, Epoch time: 0.922s.
Epoch: 160/300, Train loss: 0.3467, Val loss: 0.3549, Min val loss: 0.3541, Epoch time: 0.925s.
Epoch: 180/300, Train loss: 0.3428, Val loss: 0.3497, Min val loss: 0.3490, Epoch time: 0.859s.
Epoch: 200/300, Train loss: 0.3375, Val loss: 0.3451, Min val loss: 0.3451, Epoch time: 0.645s.
Epoch: 220/300, Train loss: 0.3343, Val loss: 0.3423, Min val loss: 0.3423, Epoch time: 0.672s.
Epoch: 240/300, Train loss: 0.3297, Val loss: 0.3415, Min val loss: 0.3398, Epoch time: 0.717s.
Epoch: 260/300, Train loss: 0.3241, Val loss: 0.3431, Min val loss: 0.3391, Epoch time: 0.924s.
Epoch: 280/300, Train loss: 0.3212, Val loss: 0.3373, Min val loss: 0.3373, Epoch time: 0.800s.
Epoch: 300/300, Train loss: 0.3201, Val loss: 0.3380, Min val loss: 0.3362, Epoch time: 0.754s.
`Trainer.fit` stopped: `max_epochs=300` reached.
2023-09-23 20:40:10,281 - {pytorch_tabular.tabular_model:584} - INFO - Training the model completed
2023-09-23 20:40:10,281 - {pytorch_tabular.tabular_model:1258} - INFO - Loading the best model
/home/xlluo/anaconda3/envs/tabular_ensemble/lib/python3.10/site-packages/pytorch_lightning/utilities/cloud_io.py:33: LightningDeprecationWarning: `pytorch_lightning.utilities.cloud_io.get_filesystem` has been deprecated in v1.8.0 and will be removed in v2.0.0. Please use `lightning_fabric.utilities.cloud_io.get_filesystem` instead.
  rank_zero_deprecation(
Training log_loss loss: 0.31245
Validation log_loss loss: 0.33618
Testing log_loss loss: 0.33183
Trainer saved. To load the trainer, run trainer = load_trainer(path=&#39;/tmp/tmpdriivjp7/output/adult/2023-09-23-20-35-54-0_UserInputConfig/trainer.pkl&#39;)

-------------PytorchTabular End-------------


-------------Run WideDeep-------------

Training TabNet
Epoch: 1/300, Train loss: 1.1847, Val loss: 0.9394, Min val loss: 0.9394
Epoch: 21/300, Train loss: 0.4870, Val loss: 0.4681, Min val loss: 0.4681
Epoch: 41/300, Train loss: 0.4204, Val loss: 0.4097, Min val loss: 0.4097
Epoch: 61/300, Train loss: 0.3905, Val loss: 0.3855, Min val loss: 0.3852
Epoch: 81/300, Train loss: 0.3784, Val loss: 0.3750, Min val loss: 0.3750
Epoch: 101/300, Train loss: 0.3657, Val loss: 0.3704, Min val loss: 0.3704
Epoch: 121/300, Train loss: 0.3642, Val loss: 0.3670, Min val loss: 0.3659
Epoch: 141/300, Train loss: 0.3569, Val loss: 0.3619, Min val loss: 0.3619
Epoch: 161/300, Train loss: 0.3502, Val loss: 0.3583, Min val loss: 0.3576
Epoch: 181/300, Train loss: 0.3462, Val loss: 0.3555, Min val loss: 0.3555
Epoch: 201/300, Train loss: 0.3392, Val loss: 0.3508, Min val loss: 0.3505
Epoch: 221/300, Train loss: 0.3366, Val loss: 0.3490, Min val loss: 0.3481
Epoch: 241/300, Train loss: 0.3314, Val loss: 0.3454, Min val loss: 0.3446
Epoch: 261/300, Train loss: 0.3282, Val loss: 0.3423, Min val loss: 0.3419
Epoch: 281/300, Train loss: 0.3239, Val loss: 0.3412, Min val loss: 0.3412
Restoring model weights from the end of the best epoch
Training log_loss loss: 0.31902
Validation log_loss loss: 0.33853
Testing log_loss loss: 0.33015
Trainer saved. To load the trainer, run trainer = load_trainer(path=&#39;/tmp/tmpdriivjp7/output/adult/2023-09-23-20-35-54-0_UserInputConfig/trainer.pkl&#39;)

-------------WideDeep End-------------


-------------Run TabNetFromAbstract-------------

Training TabNet
/home/xlluo/anaconda3/envs/tabular_ensemble/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda
  warnings.warn(f&#34;Device used : {self.device}&#34;)
epoch 0  | loss: 0.60097 | val_0_logloss: 0.60262 |  0:00:00s
epoch 20 | loss: 0.40801 | val_0_logloss: 0.40382 |  0:00:09s
epoch 40 | loss: 0.37704 | val_0_logloss: 0.38881 |  0:00:18s
epoch 60 | loss: 0.36342 | val_0_logloss: 0.37497 |  0:00:28s
epoch 80 | loss: 0.34997 | val_0_logloss: 0.36238 |  0:00:37s
epoch 100| loss: 0.34382 | val_0_logloss: 0.35627 |  0:00:46s
epoch 120| loss: 0.33903 | val_0_logloss: 0.35555 |  0:00:55s
epoch 140| loss: 0.33296 | val_0_logloss: 0.3552  |  0:01:04s
epoch 160| loss: 0.32897 | val_0_logloss: 0.35173 |  0:01:13s
epoch 180| loss: 0.32246 | val_0_logloss: 0.34939 |  0:01:23s
epoch 200| loss: 0.31779 | val_0_logloss: 0.34762 |  0:01:32s
epoch 220| loss: 0.31467 | val_0_logloss: 0.34598 |  0:01:41s
epoch 240| loss: 0.31257 | val_0_logloss: 0.34283 |  0:01:50s
epoch 260| loss: 0.31309 | val_0_logloss: 0.34439 |  0:01:59s
epoch 280| loss: 0.30807 | val_0_logloss: 0.33702 |  0:02:08s
Stop training because you reached max_epochs = 300 with best_epoch = 296 and best_val_0_logloss = 0.33691
/home/xlluo/anaconda3/envs/tabular_ensemble/lib/python3.10/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
Training log_loss loss: 0.29280
Validation log_loss loss: 0.33691
Testing log_loss loss: 0.33218
Trainer saved. To load the trainer, run trainer = load_trainer(path=&#39;/tmp/tmpdriivjp7/output/adult/2023-09-23-20-35-54-0_UserInputConfig/trainer.pkl&#39;)

-------------TabNetFromAbstract End-------------


-------------Run TabNetFromTorch-------------

Training TabNet
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device (&#39;NVIDIA GeForce RTX 3090&#39;) that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision(&#39;medium&#39; | &#39;high&#39;)` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name                | Type              | Params
----------------------------------------------------------
0 | default_loss_fn     | BCEWithLogitsLoss | 0
1 | default_output_norm | Sigmoid           | 0
2 | network             | TabNet            | 8.0 K
----------------------------------------------------------
8.0 K     Trainable params
0         Non-trainable params
8.0 K     Total params
0.032     Total estimated model params size (MB)
Epoch: 1/300, Train loss: 0.6781, Val loss: 0.6334, Min val loss: 0.6334, Min ES val loss: 0.6334, Epoch time: 0.487s.
Epoch: 20/300, Train loss: 0.4223, Val loss: 0.4170, Min val loss: 0.4170, Min ES val loss: 0.4170, Epoch time: 0.479s.
Epoch: 40/300, Train loss: 0.3829, Val loss: 0.3858, Min val loss: 0.3844, Min ES val loss: 0.3844, Epoch time: 0.567s.
Epoch: 60/300, Train loss: 0.3665, Val loss: 0.3691, Min val loss: 0.3691, Min ES val loss: 0.3691, Epoch time: 0.477s.
Epoch: 80/300, Train loss: 0.3556, Val loss: 0.3600, Min val loss: 0.3600, Min ES val loss: 0.3600, Epoch time: 0.480s.
Epoch: 100/300, Train loss: 0.3466, Val loss: 0.3559, Min val loss: 0.3547, Min ES val loss: 0.3547, Epoch time: 0.481s.
Epoch: 120/300, Train loss: 0.3414, Val loss: 0.3542, Min val loss: 0.3533, Min ES val loss: 0.3533, Epoch time: 0.478s.
Epoch: 140/300, Train loss: 0.3382, Val loss: 0.3500, Min val loss: 0.3498, Min ES val loss: 0.3498, Epoch time: 0.556s.
Epoch: 160/300, Train loss: 0.3307, Val loss: 0.3470, Min val loss: 0.3470, Min ES val loss: 0.3470, Epoch time: 0.481s.
Epoch: 180/300, Train loss: 0.3280, Val loss: 0.3439, Min val loss: 0.3439, Min ES val loss: 0.3439, Epoch time: 0.480s.
Epoch: 200/300, Train loss: 0.3249, Val loss: 0.3408, Min val loss: 0.3402, Min ES val loss: 0.3402, Epoch time: 0.484s.
Epoch: 220/300, Train loss: 0.3206, Val loss: 0.3378, Min val loss: 0.3375, Min ES val loss: 0.3375, Epoch time: 0.480s.
Epoch: 240/300, Train loss: 0.3164, Val loss: 0.3386, Min val loss: 0.3370, Min ES val loss: 0.3370, Epoch time: 0.561s.
Epoch: 260/300, Train loss: 0.3152, Val loss: 0.3374, Min val loss: 0.3342, Min ES val loss: 0.3342, Epoch time: 0.487s.
Epoch: 280/300, Train loss: 0.3144, Val loss: 0.3327, Min val loss: 0.3327, Min ES val loss: 0.3327, Epoch time: 0.558s.
Epoch: 300/300, Train loss: 0.3124, Val loss: 0.3332, Min val loss: 0.3327, Min ES val loss: 0.3327, Epoch time: 0.483s.
`Trainer.fit` stopped: `max_epochs=300` reached.
Training log_loss loss: 0.30412
Validation log_loss loss: 0.33268
Testing log_loss loss: 0.33141
Trainer saved. To load the trainer, run trainer = load_trainer(path=&#39;/tmp/tmpdriivjp7/output/adult/2023-09-23-20-35-54-0_UserInputConfig/trainer.pkl&#39;)

-------------TabNetFromTorch End-------------

PytorchTabular metrics
TabNet 1/1
WideDeep metrics
TabNet 1/1
TabNetFromAbstract metrics
TabNet 1/1
TabNetFromTorch metrics
TabNet 1/1
Trainer saved. To load the trainer, run trainer = load_trainer(path=&#39;/tmp/tmpdriivjp7/output/adult/2023-09-23-20-35-54-0_UserInputConfig/trainer.pkl&#39;)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Program</th>
      <th>Model</th>
      <th>Training F1_SCORE</th>
      <th>Training PRECISION_SCORE</th>
      <th>Training RECALL_SCORE</th>
      <th>Training JACCARD_SCORE</th>
      <th>Training ACCURACY_SCORE</th>
      <th>Training BALANCED_ACCURACY_SCORE</th>
      <th>Training COHEN_KAPPA_SCORE</th>
      <th>Training HAMMING_LOSS</th>
      <th>...</th>
      <th>Validation ACCURACY_SCORE</th>
      <th>Validation BALANCED_ACCURACY_SCORE</th>
      <th>Validation COHEN_KAPPA_SCORE</th>
      <th>Validation HAMMING_LOSS</th>
      <th>Validation MATTHEWS_CORRCOEF</th>
      <th>Validation ZERO_ONE_LOSS</th>
      <th>Validation ROC_AUC_SCORE</th>
      <th>Validation LOG_LOSS</th>
      <th>Validation BRIER_SCORE_LOSS</th>
      <th>Validation AVERAGE_PRECISION_SCORE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>PytorchTabular</td>
      <td>TabNet</td>
      <td>0.663081</td>
      <td>0.757096</td>
      <td>0.589836</td>
      <td>0.495977</td>
      <td>0.855702</td>
      <td>0.764917</td>
      <td>0.573066</td>
      <td>0.144298</td>
      <td>...</td>
      <td>0.847359</td>
      <td>0.753237</td>
      <td>0.548046</td>
      <td>0.152641</td>
      <td>0.555038</td>
      <td>0.152641</td>
      <td>0.895756</td>
      <td>0.336181</td>
      <td>0.106143</td>
      <td>0.855486</td>
    </tr>
    <tr>
      <th>1</th>
      <td>TabNetFromTorch</td>
      <td>TabNet</td>
      <td>0.672948</td>
      <td>0.754224</td>
      <td>0.607485</td>
      <td>0.507100</td>
      <td>0.857852</td>
      <td>0.772360</td>
      <td>0.583483</td>
      <td>0.142148</td>
      <td>...</td>
      <td>0.846744</td>
      <td>0.759372</td>
      <td>0.552975</td>
      <td>0.153256</td>
      <td>0.557504</td>
      <td>0.153256</td>
      <td>0.896746</td>
      <td>0.332678</td>
      <td>0.105144</td>
      <td>0.860627</td>
    </tr>
    <tr>
      <th>2</th>
      <td>WideDeep</td>
      <td>TabNet</td>
      <td>0.664649</td>
      <td>0.734311</td>
      <td>0.607059</td>
      <td>0.497734</td>
      <td>0.852529</td>
      <td>0.768709</td>
      <td>0.571219</td>
      <td>0.147471</td>
      <td>...</td>
      <td>0.845055</td>
      <td>0.762183</td>
      <td>0.552930</td>
      <td>0.154945</td>
      <td>0.556004</td>
      <td>0.154945</td>
      <td>0.894150</td>
      <td>0.338528</td>
      <td>0.107826</td>
      <td>0.852035</td>
    </tr>
    <tr>
      <th>3</th>
      <td>TabNetFromAbstract</td>
      <td>TabNet</td>
      <td>0.692785</td>
      <td>0.753562</td>
      <td>0.641080</td>
      <td>0.529970</td>
      <td>0.863124</td>
      <td>0.787303</td>
      <td>0.605467</td>
      <td>0.136876</td>
      <td>...</td>
      <td>0.845670</td>
      <td>0.766947</td>
      <td>0.558357</td>
      <td>0.154330</td>
      <td>0.560548</td>
      <td>0.154330</td>
      <td>0.896387</td>
      <td>0.336909</td>
      <td>0.106059</td>
      <td>0.857159</td>
    </tr>
  </tbody>
</table>
<p>4 rows × 44 columns</p>
</div></div>
</div>
<p>Finally the multiclass classification task:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">iris_columns</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;sepal length&quot;</span><span class="p">,</span>
    <span class="s2">&quot;sepal width&quot;</span><span class="p">,</span>
    <span class="s2">&quot;petal length&quot;</span><span class="p">,</span>
    <span class="s2">&quot;petal width&quot;</span><span class="p">,</span>
    <span class="s2">&quot;class&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">cfg</span> <span class="o">=</span> <span class="n">UserConfig</span><span class="o">.</span><span class="n">from_uci</span><span class="p">(</span><span class="s2">&quot;Iris&quot;</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="n">iris_columns</span><span class="p">,</span> <span class="n">datafile_name</span><span class="o">=</span><span class="s2">&quot;iris&quot;</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">load_config</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">add_modelbases</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">PytorchTabular</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">model_subset</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;TabNet&quot;</span><span class="p">]),</span>
        <span class="n">WideDeep</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">model_subset</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;TabNet&quot;</span><span class="p">]),</span>
        <span class="n">TabNetFromAbstract</span><span class="p">(</span><span class="n">trainer</span><span class="p">),</span>
        <span class="n">TabNetFromTorch</span><span class="p">(</span><span class="n">trainer</span><span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">stderr_to_stdout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">get_leaderboard</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Downloading https://archive.ics.uci.edu/static/public/53/iris.zip to /tmp/tmpdriivjp7/data/Iris.zip
The project will be saved to /tmp/tmpdriivjp7/output/iris/2023-09-23-20-47-59-0_UserInputConfig
Dataset size: 90 30 30
Data saved to /tmp/tmpdriivjp7/output/iris/2023-09-23-20-47-59-0_UserInputConfig (data.csv and tabular_data.csv).
{&#39;some_param&#39;: 1.1, &#39;program&#39;: None, &#39;model_subset&#39;: None, &#39;exclude_models&#39;: None, &#39;store_in_harddisk&#39;: True}

-------------Run PytorchTabular-------------

Training TabNet
Global seed set to 42
2023-09-23 20:47:59,284 - {pytorch_tabular.tabular_model:473} - INFO - Preparing the DataLoaders
2023-09-23 20:47:59,284 - {pytorch_tabular.tabular_datamodule:290} - INFO - Setting up the datamodule for classification task
2023-09-23 20:47:59,291 - {pytorch_tabular.tabular_model:521} - INFO - Preparing the Model: TabNetModel
2023-09-23 20:47:59,303 - {pytorch_tabular.tabular_model:268} - INFO - Preparing the Trainer
/home/xlluo/anaconda3/envs/tabular_ensemble/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:589: LightningDeprecationWarning: The Trainer argument `auto_select_gpus` has been deprecated in v1.9.0 and will be removed in v2.0.0. Please use the function `pytorch_lightning.accelerators.find_usable_cuda_devices` instead.
  rank_zero_deprecation(
Auto select gpus: [0]
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2023-09-23 20:47:59,315 - {pytorch_tabular.tabular_model:582} - INFO - Training Started
You are using a CUDA device (&#39;NVIDIA GeForce RTX 3090&#39;) that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision(&#39;medium&#39; | &#39;high&#39;)` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name             | Type             | Params
------------------------------------------------------
0 | _embedding_layer | Identity         | 0
1 | _backbone        | TabNetBackbone   | 5.9 K
2 | _head            | Identity         | 0
3 | loss             | CrossEntropyLoss | 0
------------------------------------------------------
5.9 K     Trainable params
0         Non-trainable params
5.9 K     Total params
0.024     Total estimated model params size (MB)
Epoch: 1/300, Train loss: 1.3527, Val loss: 3.6434, Min val loss: 3.6434, Epoch time: 0.022s.
Epoch: 20/300, Train loss: 0.8093, Val loss: 2.1175, Min val loss: 2.1175, Epoch time: 0.019s.
Epoch: 40/300, Train loss: 0.5522, Val loss: 1.1805, Min val loss: 1.1805, Epoch time: 0.018s.
Epoch: 60/300, Train loss: 0.3874, Val loss: 0.8075, Min val loss: 0.8075, Epoch time: 0.018s.
Epoch: 80/300, Train loss: 0.2738, Val loss: 0.6783, Min val loss: 0.6783, Epoch time: 0.019s.
Epoch: 100/300, Train loss: 0.2028, Val loss: 0.6126, Min val loss: 0.6126, Epoch time: 0.019s.
Epoch: 120/300, Train loss: 0.1537, Val loss: 0.5558, Min val loss: 0.5558, Epoch time: 0.019s.
Epoch: 140/300, Train loss: 0.1162, Val loss: 0.4934, Min val loss: 0.4934, Epoch time: 0.019s.
Epoch: 160/300, Train loss: 0.0899, Val loss: 0.4366, Min val loss: 0.4366, Epoch time: 0.019s.
Epoch: 180/300, Train loss: 0.0703, Val loss: 0.3954, Min val loss: 0.3954, Epoch time: 0.019s.
Epoch: 200/300, Train loss: 0.0560, Val loss: 0.3592, Min val loss: 0.3592, Epoch time: 0.019s.
Epoch: 220/300, Train loss: 0.0453, Val loss: 0.3398, Min val loss: 0.3398, Epoch time: 0.020s.
Epoch: 240/300, Train loss: 0.0373, Val loss: 0.3234, Min val loss: 0.3234, Epoch time: 0.019s.
Epoch: 260/300, Train loss: 0.0309, Val loss: 0.3124, Min val loss: 0.3124, Epoch time: 0.019s.
Epoch: 280/300, Train loss: 0.0260, Val loss: 0.3075, Min val loss: 0.3075, Epoch time: 0.019s.
Epoch: 300/300, Train loss: 0.0222, Val loss: 0.3039, Min val loss: 0.3033, Epoch time: 0.019s.
`Trainer.fit` stopped: `max_epochs=300` reached.
2023-09-23 20:48:07,783 - {pytorch_tabular.tabular_model:584} - INFO - Training the model completed
2023-09-23 20:48:07,783 - {pytorch_tabular.tabular_model:1258} - INFO - Loading the best model
/home/xlluo/anaconda3/envs/tabular_ensemble/lib/python3.10/site-packages/pytorch_lightning/utilities/cloud_io.py:33: LightningDeprecationWarning: `pytorch_lightning.utilities.cloud_io.get_filesystem` has been deprecated in v1.8.0 and will be removed in v2.0.0. Please use `lightning_fabric.utilities.cloud_io.get_filesystem` instead.
  rank_zero_deprecation(
Training log_loss loss: 0.02973
Validation log_loss loss: 0.30334
Testing log_loss loss: 0.08998
Trainer saved. To load the trainer, run trainer = load_trainer(path=&#39;/tmp/tmpdriivjp7/output/iris/2023-09-23-20-47-59-0_UserInputConfig/trainer.pkl&#39;)

-------------PytorchTabular End-------------


-------------Run WideDeep-------------

Training TabNet
Epoch: 1/300, Train loss: 1.3533, Val loss: 3.6438, Min val loss: 3.6438
Epoch: 21/300, Train loss: 0.7912, Val loss: 2.0427, Min val loss: 2.0427
Epoch: 41/300, Train loss: 0.5492, Val loss: 1.1557, Min val loss: 1.1557
Epoch: 61/300, Train loss: 0.3919, Val loss: 0.8098, Min val loss: 0.8098
Epoch: 81/300, Train loss: 0.2776, Val loss: 0.6744, Min val loss: 0.6744
Epoch: 101/300, Train loss: 0.2061, Val loss: 0.6021, Min val loss: 0.6021
Epoch: 121/300, Train loss: 0.1560, Val loss: 0.5320, Min val loss: 0.5320
Epoch: 141/300, Train loss: 0.1184, Val loss: 0.4719, Min val loss: 0.4719
Epoch: 161/300, Train loss: 0.0917, Val loss: 0.4163, Min val loss: 0.4163
Epoch: 181/300, Train loss: 0.0720, Val loss: 0.3811, Min val loss: 0.3811
Epoch: 201/300, Train loss: 0.0574, Val loss: 0.3475, Min val loss: 0.3475
Epoch: 221/300, Train loss: 0.0466, Val loss: 0.3298, Min val loss: 0.3298
Epoch: 241/300, Train loss: 0.0386, Val loss: 0.3141, Min val loss: 0.3141
Epoch: 261/300, Train loss: 0.0324, Val loss: 0.3057, Min val loss: 0.3057
Epoch: 281/300, Train loss: 0.0276, Val loss: 0.3003, Min val loss: 0.3003
Restoring model weights from the end of the best epoch
Training log_loss loss: 0.03166
Validation log_loss loss: 0.29784
Testing log_loss loss: 0.08862
Trainer saved. To load the trainer, run trainer = load_trainer(path=&#39;/tmp/tmpdriivjp7/output/iris/2023-09-23-20-47-59-0_UserInputConfig/trainer.pkl&#39;)

-------------WideDeep End-------------


-------------Run TabNetFromAbstract-------------

Training TabNet
/home/xlluo/anaconda3/envs/tabular_ensemble/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda
  warnings.warn(f&#34;Device used : {self.device}&#34;)
epoch 0  | loss: 1.56647 | val_0_logloss: 2.77843 |  0:00:00s
epoch 20 | loss: 0.84741 | val_0_logloss: 1.56011 |  0:00:00s
epoch 40 | loss: 0.57801 | val_0_logloss: 1.30489 |  0:00:00s
epoch 60 | loss: 0.42103 | val_0_logloss: 0.94241 |  0:00:01s
epoch 80 | loss: 0.30948 | val_0_logloss: 0.82352 |  0:00:01s
epoch 100| loss: 0.22945 | val_0_logloss: 0.67582 |  0:00:01s
epoch 120| loss: 0.17005 | val_0_logloss: 0.56843 |  0:00:02s
epoch 140| loss: 0.12258 | val_0_logloss: 0.43708 |  0:00:02s
epoch 160| loss: 0.08765 | val_0_logloss: 0.37523 |  0:00:03s
epoch 180| loss: 0.06136 | val_0_logloss: 0.32675 |  0:00:03s
epoch 200| loss: 0.04456 | val_0_logloss: 0.29909 |  0:00:03s
epoch 220| loss: 0.03383 | val_0_logloss: 0.28244 |  0:00:04s
epoch 240| loss: 0.02584 | val_0_logloss: 0.26869 |  0:00:04s
epoch 260| loss: 0.02049 | val_0_logloss: 0.2337  |  0:00:05s
epoch 280| loss: 0.01678 | val_0_logloss: 0.20078 |  0:00:05s
Stop training because you reached max_epochs = 300 with best_epoch = 299 and best_val_0_logloss = 0.17915
/home/xlluo/anaconda3/envs/tabular_ensemble/lib/python3.10/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
Training log_loss loss: 0.02356
Validation log_loss loss: 0.17915
Testing log_loss loss: 0.06823
Trainer saved. To load the trainer, run trainer = load_trainer(path=&#39;/tmp/tmpdriivjp7/output/iris/2023-09-23-20-47-59-0_UserInputConfig/trainer.pkl&#39;)

-------------TabNetFromAbstract End-------------


-------------Run TabNetFromTorch-------------

Training TabNet
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device (&#39;NVIDIA GeForce RTX 3090&#39;) that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision(&#39;medium&#39; | &#39;high&#39;)` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name                | Type             | Params
---------------------------------------------------------
0 | default_loss_fn     | CrossEntropyLoss | 0
1 | default_output_norm | Softmax          | 0
2 | network             | TabNet           | 5.9 K
---------------------------------------------------------
5.9 K     Trainable params
0         Non-trainable params
5.9 K     Total params
0.024     Total estimated model params size (MB)
Epoch: 1/300, Train loss: 1.3527, Val loss: 3.6726, Min val loss: 3.6726, Min ES val loss: 3.6726, Epoch time: 0.020s.
Epoch: 20/300, Train loss: 0.8086, Val loss: 2.0715, Min val loss: 2.0715, Min ES val loss: 2.0715, Epoch time: 0.017s.
Epoch: 40/300, Train loss: 0.5588, Val loss: 1.1552, Min val loss: 1.1552, Min ES val loss: 1.1552, Epoch time: 0.017s.
Epoch: 60/300, Train loss: 0.3996, Val loss: 0.8152, Min val loss: 0.8152, Min ES val loss: 0.8152, Epoch time: 0.017s.
Epoch: 80/300, Train loss: 0.2837, Val loss: 0.6745, Min val loss: 0.6745, Min ES val loss: 0.6745, Epoch time: 0.017s.
Epoch: 100/300, Train loss: 0.2098, Val loss: 0.6031, Min val loss: 0.6031, Min ES val loss: 0.6031, Epoch time: 0.018s.
Epoch: 120/300, Train loss: 0.1586, Val loss: 0.5270, Min val loss: 0.5270, Min ES val loss: 0.5270, Epoch time: 0.017s.
Epoch: 140/300, Train loss: 0.1203, Val loss: 0.4580, Min val loss: 0.4580, Min ES val loss: 0.4580, Epoch time: 0.020s.
Epoch: 160/300, Train loss: 0.0926, Val loss: 0.4144, Min val loss: 0.4144, Min ES val loss: 0.4144, Epoch time: 0.017s.
Epoch: 180/300, Train loss: 0.0724, Val loss: 0.3771, Min val loss: 0.3771, Min ES val loss: 0.3771, Epoch time: 0.017s.
Epoch: 200/300, Train loss: 0.0578, Val loss: 0.3446, Min val loss: 0.3446, Min ES val loss: 0.3446, Epoch time: 0.017s.
Epoch: 220/300, Train loss: 0.0467, Val loss: 0.3253, Min val loss: 0.3253, Min ES val loss: 0.3253, Epoch time: 0.016s.
Epoch: 240/300, Train loss: 0.0385, Val loss: 0.3145, Min val loss: 0.3145, Min ES val loss: 0.3145, Epoch time: 0.017s.
Epoch: 260/300, Train loss: 0.0322, Val loss: 0.3053, Min val loss: 0.3053, Min ES val loss: 0.3053, Epoch time: 0.016s.
Epoch: 280/300, Train loss: 0.0273, Val loss: 0.2999, Min val loss: 0.2998, Min ES val loss: 0.2998, Epoch time: 0.016s.
Epoch: 300/300, Train loss: 0.0235, Val loss: 0.2978, Min val loss: 0.2964, Min ES val loss: 0.2964, Epoch time: 0.016s.
`Trainer.fit` stopped: `max_epochs=300` reached.
Training log_loss loss: 0.03299
Validation log_loss loss: 0.29635
Testing log_loss loss: 0.10517
Trainer saved. To load the trainer, run trainer = load_trainer(path=&#39;/tmp/tmpdriivjp7/output/iris/2023-09-23-20-47-59-0_UserInputConfig/trainer.pkl&#39;)

-------------TabNetFromTorch End-------------

PytorchTabular metrics
TabNet 1/1
WideDeep metrics
TabNet 1/1
TabNetFromAbstract metrics
TabNet 1/1
TabNetFromTorch metrics
TabNet 1/1
Trainer saved. To load the trainer, run trainer = load_trainer(path=&#39;/tmp/tmpdriivjp7/output/iris/2023-09-23-20-47-59-0_UserInputConfig/trainer.pkl&#39;)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Program</th>
      <th>Model</th>
      <th>Training ACCURACY_SCORE</th>
      <th>Training BALANCED_ACCURACY_SCORE</th>
      <th>Training COHEN_KAPPA_SCORE</th>
      <th>Training HAMMING_LOSS</th>
      <th>Training MATTHEWS_CORRCOEF</th>
      <th>Training ZERO_ONE_LOSS</th>
      <th>Training PRECISION_SCORE_MACRO</th>
      <th>Training PRECISION_SCORE_MICRO</th>
      <th>...</th>
      <th>Validation F1_SCORE_MICRO</th>
      <th>Validation F1_SCORE_WEIGHTED</th>
      <th>Validation JACCARD_SCORE_MACRO</th>
      <th>Validation JACCARD_SCORE_MICRO</th>
      <th>Validation JACCARD_SCORE_WEIGHTED</th>
      <th>Validation TOP_K_ACCURACY_SCORE</th>
      <th>Validation LOG_LOSS</th>
      <th>Validation ROC_AUC_SCORE_OVR_MACRO</th>
      <th>Validation ROC_AUC_SCORE_OVR_WEIGHTED</th>
      <th>Validation ROC_AUC_SCORE_OVO</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>PytorchTabular</td>
      <td>TabNet</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>...</td>
      <td>0.933333</td>
      <td>0.934656</td>
      <td>0.888889</td>
      <td>0.875000</td>
      <td>0.88000</td>
      <td>1.0</td>
      <td>0.303345</td>
      <td>0.975059</td>
      <td>0.966566</td>
      <td>0.972956</td>
    </tr>
    <tr>
      <th>1</th>
      <td>WideDeep</td>
      <td>TabNet</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>...</td>
      <td>0.933333</td>
      <td>0.934656</td>
      <td>0.888889</td>
      <td>0.875000</td>
      <td>0.88000</td>
      <td>1.0</td>
      <td>0.297838</td>
      <td>0.980985</td>
      <td>0.975455</td>
      <td>0.979940</td>
    </tr>
    <tr>
      <th>2</th>
      <td>TabNetFromAbstract</td>
      <td>TabNet</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>...</td>
      <td>0.900000</td>
      <td>0.901217</td>
      <td>0.837500</td>
      <td>0.818182</td>
      <td>0.82625</td>
      <td>1.0</td>
      <td>0.179150</td>
      <td>0.989874</td>
      <td>0.988788</td>
      <td>0.990417</td>
    </tr>
    <tr>
      <th>3</th>
      <td>TabNetFromTorch</td>
      <td>TabNet</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>...</td>
      <td>0.933333</td>
      <td>0.934656</td>
      <td>0.888889</td>
      <td>0.875000</td>
      <td>0.88000</td>
      <td>1.0</td>
      <td>0.296352</td>
      <td>0.980985</td>
      <td>0.975455</td>
      <td>0.979940</td>
    </tr>
  </tbody>
</table>
<p>4 rows × 71 columns</p>
</div></div>
</div>
<p>Results show that models perform much worse on the validation set than on the testing set. To get reliable results, we recommend using cross-validation to get the leaderboard:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># trainer.train(stderr_to_stdout=True)  # No need to run `train`</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">get_leaderboard</span><span class="p">(</span><span class="n">cross_validation</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">split_type</span><span class="o">=</span><span class="s2">&quot;cv&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>


                </article>
              
              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="../advanced_usage.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Advanced Usage</p>
      </div>
    </a>
    <a class="right-next"
       href="customized_model_base_advanced.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Advanced customized model base</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Example:-Implement-TabNet-as-a-model-base-from-scratch">Example: Implement TabNet as a model base from scratch</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Example:-Implement-TabNet-as-a-PyTorch-based-model">Example: Implement TabNet as a <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code>-based model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Comparison-of-different-implementations-in-other-model-bases">Comparison of different implementations in other model bases</a></li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">
  
  <div class="tocsection editthispage">
    <a href="https://github.com/ANONYMOUS/tabular_ensemble/edit/main/docs/source/examples/advanced_usage/customized_model_base.ipynb">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>

  <div class="sidebar-secondary-item">
  <div class="tocsection sourcelink">
    <a href="../../_sources/examples/advanced_usage/customized_model_base.ipynb.txt">
      <i class="fa-solid fa-file-lines"></i> Show Source
    </a>
  </div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
  <p class="copyright">
    
      © Copyright 2023, Tabular Ensemble developers.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">
  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.5.
    <br/>
  </p>
</div>
      
    </div>
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.13.3.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>